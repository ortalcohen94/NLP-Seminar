{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67e6ba05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters:\n",
    "\n",
    "SHOULD_USE_SEED = True\n",
    "RADIUS = 5000\n",
    "BATCH_SIZE = 700\n",
    "EPSILON = 0.1\n",
    "SIGMA = 0.001\n",
    "MAX_ITERS = 100\n",
    "\n",
    "NN_BATCH_SIZE = 128\n",
    "\n",
    "SAMPLES_FOR_MEASURE = 100 #If all the samples should be measured, the value should be None.\n",
    "\n",
    "RERUN_EXPERIMENT = False\n",
    "\n",
    "TRAIN_FILE = \"esnli_train.csv\"\n",
    "VAL_FILE = \"esnli_dev.csv\"\n",
    "TEST_FILE = \"esnli_test.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfac5bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from data.data_creator import data_create_SNLI\n",
    "\n",
    "if SHOULD_USE_SEED:\n",
    "    np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77e0510a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e290f6b",
   "metadata": {},
   "source": [
    "## Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3919a68e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data, labels = data_create_SNLI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4337536",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Splitting the dataset by using the original split\n",
    "\n",
    "x_train, x_val, x_test = data[TRAIN_FILE], data[VAL_FILE], data[TEST_FILE]\n",
    "y_train, y_val, y_test = labels[TRAIN_FILE], labels[VAL_FILE], labels[TEST_FILE]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1c07f7",
   "metadata": {},
   "source": [
    "## Black-box model - Multinomial Naive Bayes classifier\n",
    "\n",
    "The black box model that was used in the original article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c84857d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "data_multinomial_nb = dict()\n",
    "\n",
    "for key in data:\n",
    "    data_multinomial_nb[key] = \\\n",
    "        [first + '~ ' + second for first, second in zip(data[key]['premise'], data[key]['hypothesis'])]\n",
    "    \n",
    "x_train, x_val, x_test = data_multinomial_nb[TRAIN_FILE], data_multinomial_nb[VAL_FILE], data_multinomial_nb[TEST_FILE]\n",
    "\n",
    "vect_text = TfidfVectorizer(use_idf = False)\n",
    "x_vec_train = vect_text.fit_transform(x_train)\n",
    "\n",
    "clf = MultinomialNB().fit(x_vec_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb8a419c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf.predict(vect_text.transform(x_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52677dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy 0.5420646210119894\n"
     ]
    }
   ],
   "source": [
    "print('Val accuracy', metrics.accuracy_score(y_val, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2104fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy 0.5362377850162866\n"
     ]
    }
   ],
   "source": [
    "preds = clf.predict(vect_text.transform(x_test))\n",
    "print('Test accuracy', metrics.accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bb75a5",
   "metadata": {},
   "source": [
    "## Black-box model - Neural Network\n",
    "\n",
    "The black box model above doesn't give a good result. Therefore, we use another black box model presented in the following article:\n",
    "https://nlp.stanford.edu/pubs/snli_paper.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dba8c0a",
   "metadata": {},
   "source": [
    "### Data preperation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d13b45fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://jamesmccaffrey.wordpress.com/2021/01/04/creating-a-custom-torchtext-dataset-from-a-text-file/\n",
    "\n",
    "# from torchtext.legacy.data import Field\n",
    "# import torchtext as tt\n",
    "\n",
    "# TEXT = tt.legacy.data.Field(sequential=True,\n",
    "#   init_token='(bos)',  # start of sequence\n",
    "#   eos_token='(eos)',   # replace parens with less, greater\n",
    "#   lower=True,\n",
    "#   tokenize=tt.data.utils.get_tokenizer(\"basic_english\"),)\n",
    "# LABEL = tt.legacy.data.Field(sequential=False,\n",
    "#   use_vocab=True,\n",
    "#   unk_token=None,\n",
    "#   is_target=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e220f953",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "cwd = os.getcwd()\n",
    "data_dir = join(cwd, \"data/eSNLI\")\n",
    "df = {}\n",
    "df_names = {TRAIN_FILE : 'train', VAL_FILE: 'val', TEST_FILE: 'test'}\n",
    "for file in TRAIN_FILE, VAL_FILE, TEST_FILE:\n",
    "    data = pd.read_csv(join(data_dir, file))\n",
    "    data_sentences = data[['Sentence1', 'Sentence2', 'gold_label']]\n",
    "    data_sentences = data_sentences.dropna()\n",
    "    labelencoder = LabelEncoder()\n",
    "\n",
    "    data_sentences['gold_label_cat'] = labelencoder.fit_transform(data_sentences['gold_label'])\n",
    "    data_sentences.drop('gold_label', inplace = True, axis = 1)\n",
    "    df[df_names[file]] = data_sentences\n",
    "    \n",
    "\n",
    "\n",
    "# (train_obj, valid_obj, test_obj) = tt.legacy.data.TabularDataset.splits(\n",
    "#   path=\".//data/eSNLI/\",\n",
    "#   train=TRAIN_FILE,\n",
    "#   validation=VAL_FILE,\n",
    "#   test=TEST_FILE,\n",
    "#   skip_header = True,\n",
    "#   format='csv',\n",
    "#   filter_pred = lambda x: x.gold_label != '-',\n",
    "#   fields=[('pairID', None), ('gold_label', LABEL), ('sentence1', TEXT), ('sentence2', TEXT)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e10115c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEXT.build_vocab(train_obj.sentence1, min_freq=1, vectors='glove.6B.300d')\n",
    "# TEXT.build_vocab(train_obj.sentence2, min_freq=1, vectors='glove.6B.300d')\n",
    "# LABEL.build_vocab(train_obj.gold_label)\n",
    "# pretrained_embeddings = TEXT.vocab.vectors\n",
    "def text_preprocessing(text):\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "77f3fb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_iter, val_iter, test_iter = tt.legacy.data.BucketIterator.splits(\n",
    "#    (train_obj, valid_obj, test_obj), \n",
    "#     batch_size=NN_BATCH_SIZE, \n",
    "#     sort_key = lambda x: len(x.sentence1),\n",
    "#     sort_within_batch = False,\n",
    "#     repeat=False, \n",
    "#     device=device)\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import re\n",
    "MAX_LEN = 63\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "def preprocessing_for_bert_helper(sentences):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "#     input_ids_2 = []\n",
    "#     attention_masks_2 = []\n",
    "\n",
    "#     sentence1 = data.Sentence1.values\n",
    "#     sentence2 = data.Sentence2.values\n",
    "\n",
    "    for sent in sentences:\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=text_preprocessing(sent),  \n",
    "            add_special_tokens=True,        \n",
    "            max_length=MAX_LEN,                 \n",
    "            pad_to_max_length=True,    \n",
    "            truncation=True,\n",
    "            #return_tensors='pt',           \n",
    "            return_attention_mask=True      \n",
    "            )\n",
    "        \n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "\n",
    "\n",
    "#     for sent in sentence2:\n",
    "#         encoded_sent = tokenizer.encode_plus(\n",
    "#             text=text_preprocessing(sent),  \n",
    "#             add_special_tokens=True,        \n",
    "#             max_length=MAX_LEN,                 \n",
    "#             pad_to_max_length=True,         \n",
    "#             #return_tensors='pt',           \n",
    "#             return_attention_mask=True      \n",
    "#             )\n",
    "        \n",
    "#         input_ids_2.append(encoded_sent.get('input_ids'))\n",
    "#         attention_masks_2.append(encoded_sent.get('attention_mask'))        \n",
    "\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "#     input_ids_2 = torch.tensor(input_ids_2)\n",
    "#     attention_masks_2 = torch.tensor(attention_masks_2)\n",
    "\n",
    "    return input_ids, attention_masks\n",
    "\n",
    "def preprocessing_for_bert(sentences):\n",
    "\n",
    "    premise_input_ids, premise_attention_masks = preprocessing_for_bert_helper(sentences['Sentence1'].values)\n",
    "    hypothesis_input_ids, hypothesis_attention_masks = preprocessing_for_bert_helper(sentences['Sentence2'].values)\n",
    "    inputs = torch.cat([premise_input_ids, hypothesis_input_ids], dim = 1)\n",
    "    masks = torch.cat([premise_attention_masks, hypothesis_attention_masks], dim = 1)\n",
    "    \n",
    "    return inputs, masks\n",
    "\n",
    "\n",
    "def GetDataLoader(df):\n",
    "    inputs, masks = preprocessing_for_bert(df)\n",
    "        \n",
    "    #inputs, masks = preprocessing_for_bert(df.Sentence1.values)\n",
    "    # Convert other data types to torch.Tensor\n",
    "    labels = torch.tensor(df['gold_label_cat'].values)\n",
    "\n",
    "    # For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
    "    batch_size = 32\n",
    "\n",
    "    # Create the DataLoader for our training set\n",
    "    data = TensorDataset(inputs, masks, labels)\n",
    "    sampler = RandomSampler(data)\n",
    "    dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size)\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "dataloaders = {}\n",
    "for key, curr_df in df.items():\n",
    "    dataloaders[key] = GetDataLoader(curr_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901aeb0d",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0973f211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "import time \n",
    "\n",
    "# Create the BertClassfier class\n",
    "class BertClassifier(nn.Module):\n",
    "    \"\"\"Bert Model for Classification Tasks.\n",
    "    \"\"\"\n",
    "    def __init__(self, device, freeze_bert=False):\n",
    "        \"\"\"\n",
    "        @param    bert: a BertModel object\n",
    "        @param    classifier: a torch.nn.Module classifier\n",
    "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
    "        \"\"\"\n",
    "        super(BertClassifier, self).__init__()\n",
    "        D_in, H, D_out = 768, 50, 3\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(D_in, H),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(0.5),\n",
    "            nn.Linear(H, D_out)\n",
    "        )\n",
    "\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Feed input to BERT and the classifier to compute logits.\n",
    "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
    "                      max_length)\n",
    "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
    "                      information with shape (batch_size, max_length)\n",
    "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
    "                      num_labels)\n",
    "        \"\"\"\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "        \n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "    def predict(self, input_ids, attention_mask):\n",
    "        \"\"\"Returns the most likely sequence of tags for a sequence of words in `text_batch`.\n",
    "\n",
    "        Arguments: \n",
    "          text_batch: a tensor containing word ids of size (seq_len, batch_size) \n",
    "        Returns:\n",
    "          tag_batch: a tensor containing tag ids of size (seq_len, batch_size)\n",
    "        \"\"\"\n",
    "        logits = self.forward(input_ids, attention_mask)\n",
    "        #print(logits)\n",
    "        tag_batch = torch.argmax(logits, axis = -1)\n",
    "        return tag_batch\n",
    "\n",
    "class BertNLITrainer:\n",
    "    \n",
    "    def __init__(self,model, device):\n",
    "        self.model = model\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.device = device\n",
    "        \n",
    "    def train(self, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
    "        model = self.model\n",
    "        print(\"Start training...\\n\")\n",
    "        for epoch_i in range(epochs):\n",
    "\n",
    "            print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
    "            print(\"-\"*70)\n",
    "\n",
    "            t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "            total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "            model.train()\n",
    "\n",
    "            for step, batch in enumerate(train_dataloader):\n",
    "                batch_counts +=1\n",
    "                b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "                model.zero_grad()\n",
    "\n",
    "                logits = model(b_input_ids, b_attn_mask)\n",
    "                loss = self.loss_fn(logits, b_labels)\n",
    "                batch_loss += loss.item()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "                if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                    time_elapsed = time.time() - t0_batch\n",
    "\n",
    "                    print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "\n",
    "                    batch_loss, batch_counts = 0, 0\n",
    "                    t0_batch = time.time()\n",
    "\n",
    "            avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "            print(\"-\"*70)\n",
    "\n",
    "            if evaluation == True:\n",
    "\n",
    "                val_loss, val_accuracy = self.evaluate(val_dataloader)\n",
    "\n",
    "                time_elapsed = time.time() - t0_epoch\n",
    "\n",
    "                print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "                print(\"-\"*70)\n",
    "            print(\"\\n\")\n",
    "\n",
    "        print(\"Training complete!\")\n",
    "\n",
    "\n",
    "    def evaluate(self, val_dataloader):\n",
    "        \n",
    "        \"\"\"After the completion of each training epoch, measure the model's performance\n",
    "        on our validation set.\n",
    "        \"\"\"\n",
    "        \n",
    "        model = self.model\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        val_accuracy = []\n",
    "        val_loss = []\n",
    "\n",
    "        for batch in val_dataloader:\n",
    "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "            loss = self.loss_fn(logits, b_labels)\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "            preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "            accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "            val_accuracy.append(accuracy)\n",
    "\n",
    "        val_loss = np.mean(val_loss)\n",
    "        val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "        return val_loss, val_accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1cf13bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "from black_box_models.neural_network import NeuralNetModel\n",
    "\n",
    "# clf = NeuralNetModel(100, pretrained_embeddings, 300, 1, TEXT, LABEL).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f9b6c2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LABEL.vocab.itos\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "def initialize_model(epochs=4):\n",
    "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
    "    \"\"\"\n",
    "    bert_classifier = BertClassifier(freeze_bert=False, device = device)\n",
    "\n",
    "    bert_classifier.to(device)\n",
    "\n",
    "    optimizer = AdamW(bert_classifier.parameters(),\n",
    "                      lr=5e-5,   \n",
    "                      eps=1e-8   \n",
    "                      )\n",
    "\n",
    "    total_steps = len(dataloaders['train']) * epochs\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0, \n",
    "                                                num_training_steps=total_steps)\n",
    "    return bert_classifier, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aad2f5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   1.096927   |     -      |     -     |   5.33   \n"
     ]
    }
   ],
   "source": [
    "# clf.train_all(train_iter, val_iter, epochs = 10)\n",
    "set_seed(42)  \n",
    "bert_classifier, optimizer, scheduler = initialize_model(epochs=4)\n",
    "trainer = BertNLITrainer(bert_classifier, device)\n",
    "trainer.train(dataloaders['train'], dataloaders['val'], epochs=4, evaluation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cc2eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = clf.evaluate(test_iter)\n",
    "print('Test accuracy: ', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb1c585",
   "metadata": {},
   "source": [
    "## Instance to explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075c0997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tensor_to_text (t , FIELD):\n",
    "    if (t.dim() == 0):\n",
    "        return FIELD.vocab.itos[t.item()]\n",
    "    return ' '.join([FIELD.vocab.itos[i] for i in t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec4f2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_iter))\n",
    "\n",
    "def create_tensor_from_sentence(sentence):\n",
    "    length = len(tokenizer(sentence)) + 2\n",
    "    pad_id = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "    init_id = TEXT.vocab.stoi[TEXT.init_token]\n",
    "    eos_id = TEXT.vocab.stoi[TEXT.eos_token]\n",
    "    tensor = torch.ones((2,), dtype=torch.int64)\n",
    "    t = tensor.new_full(size = (length, 1), fill_value  = pad_id, device = device)\n",
    "    t[0, 0] = init_id\n",
    "    tokens_idx = torch.LongTensor([TEXT.vocab.stoi[token] for token in tokenizer(sentence)])\n",
    "    t[1 : len(tokens_idx) + 1, 0] = tokens_idx\n",
    "    t[len(tokens_idx) + 1, 0] = eos_id\n",
    "    return t\n",
    "\n",
    "class Instance:\n",
    "    \n",
    "    def __init__(self, sentence1, sentence2):\n",
    "        self.sentence1 = sentence1\n",
    "        self.sentence2 = sentence2\n",
    "\n",
    "def transform_func(x):\n",
    "    splitted_x = x.split('*')\n",
    "    premise = splitted_x[0]\n",
    "    hypothesis = splitted_x[1]\n",
    "    t_premise = create_tensor_from_sentence(premise)\n",
    "    t_hypothesis = create_tensor_from_sentence(hypothesis)\n",
    "    return Instance(t_premise, t_hypothesis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217c2ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(x):\n",
    "    return x.split()\n",
    "\n",
    "if RERUN_EXPERIMENT:\n",
    "    for idx in range(len(x_test)):\n",
    "        x_explain = x_test[idx]\n",
    "        premise = x_test[idx].split('~')[0]\n",
    "        hypothesis = x_test[idx].split('~')[1]\n",
    "    #     \n",
    "        x_explain = premise + \" * \" + hypothesis \n",
    "        if(convert_tensor_to_text(clf.predict(transform_func(x_explain))[0], LABEL) == y_test[idx] and y_test[idx] != 'neutral'):\n",
    "            print('premise to explain: ', premise)\n",
    "            print('hypothesis to explain: ',hypothesis)\n",
    "            print('Predicted class: ', convert_tensor_to_text(clf.predict(transform_func(x_explain))[0], LABEL))\n",
    "            print('True class: ', y_test[idx])\n",
    "            break\n",
    "\n",
    "else:   \n",
    "    premise = \"This church choir sings to the masses as they sing joyous songs from the book at a church.\"\n",
    "    hypothesis = \"The church is filled with song.\"\n",
    "    x_explain = premise + \" * \" + hypothesis \n",
    "    print('premise to explain: ', premise)\n",
    "    print('hypothesis to explain: ',hypothesis)\n",
    "    print('Predicted class: ', convert_tensor_to_text(clf.predict(transform_func(x_explain))[0], LABEL))\n",
    "    print('True class: ', y_test[1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c214e6c8",
   "metadata": {},
   "source": [
    "## Building MeLime model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be40a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dl_train = [tokenizer(x) for x in x_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7647c69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_models.word2vec_gen import Word2VecGen, Word2VecEncoder\n",
    "#The radius is <radius> most similar words\n",
    "encoder = Word2VecEncoder(dl_train)\n",
    "generator = Word2VecGen(encoder = encoder, corpus = x_train, radius = RADIUS, tokenizer = tokenizer,\n",
    "                       tokens_not_to_sample = ['*', '.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3c8433",
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpretable_local_models.statistics_model_nli import StatisticsLocalModelNLI\n",
    "y_p_explain = max(clf.predict_proba(transform_func(x_explain))[0]).item()\n",
    "print('Probability for the predicted label: ', y_p_explain)\n",
    "explainer_model = StatisticsLocalModelNLI(y_p_explain, len(tokenizer(x_explain)), tokenizer, \n",
    "                                       len(tokenizer(premise)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a880e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MeLime.model import MeLimeModel\n",
    "from torch import tensor\n",
    "\n",
    "\n",
    "model = MeLimeModel(black_box_model = clf,gen_model =generator, batch_size = BATCH_SIZE, epsilon_c = EPSILON, \n",
    "                    sigma = SIGMA, explainer_model = explainer_model, transform_func = transform_func, \n",
    "                    max_iters = MAX_ITERS, tokenizer = tokenizer)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b988a94",
   "metadata": {},
   "source": [
    "## Explaining the instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c56f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "res, sentences_with_probs = model.forward(x_explain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ba51b8",
   "metadata": {},
   "source": [
    "## Plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c15fce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ax = StatisticsLocalModelNLI.plot_explaination(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7616e90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "premise_res = []\n",
    "hypothesis_res = []\n",
    "did_finish_premise = False\n",
    "for word, stat in res:\n",
    "    if word == '*':\n",
    "        did_finish_premise = True\n",
    "        continue\n",
    "    if did_finish_premise:\n",
    "        hypothesis_res.append((word, stat))\n",
    "        continue\n",
    "    premise_res.append((word, stat))\n",
    "print(\"Premise plot:\")\n",
    "StatisticsLocalModelNLI.plot_sentence_heatmap(premise_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21005ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hypothesis plot:\")\n",
    "StatisticsLocalModelNLI.plot_sentence_heatmap(hypothesis_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698add19",
   "metadata": {},
   "source": [
    "## Plotting most favorable and contrary samples phrases:\n",
    "\n",
    "Favorable sentence - a generated sentence using Word2VecGen that improves the model's confidence in its \n",
    "prediction on the original sentence.\n",
    "\n",
    "Contrary samples - a generated sentence using Word2VecGen that decrease the model's confidence in its prediction on the original sentence and <b>might even change its prediction on the generated sentence</b>.\n",
    "\n",
    "### Most contrary samples phrases:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9567648b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(sentences_with_probs, key = lambda x: x[1])[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f946d2",
   "metadata": {},
   "source": [
    "### Most favorable samples phrases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd709af",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sorted(sentences_with_probs, key = lambda x: x[1], reverse = True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f9eb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MeLime.measures import calc_f1_esnli\n",
    "\n",
    "#F1_score = 0.40293040293040294\n",
    "if RERUN_EXPERIMENT or True:\n",
    "    F1_score = calc_f1_esnli(convert_tensor_to_text, clf, transform_func, LABEL, y_p_explain, tokenizer, encoder, \n",
    "                                          x_train, RADIUS, BATCH_SIZE, EPSILON, SIGMA, MAX_ITERS, SAMPLES_FOR_MEASURE)\n",
    "print(\"F1 score: \", F1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f35b0f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1942e92a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
