{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "67e6ba05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters:\n",
    "\n",
    "SHOULD_USE_SEED = True\n",
    "RADIUS = 10000\n",
    "BATCH_SIZE = 700\n",
    "EPSILON = 0.1\n",
    "SIGMA = 0.0001\n",
    "MAX_ITERS = 100\n",
    "\n",
    "NN_BATCH_SIZE = 1024\n",
    "\n",
    "TRAIN_FILE = \"snli_1.0_train.txt\"\n",
    "VAL_FILE = \"snli_1.0_dev.txt\"\n",
    "TEST_FILE = \"snli_1.0_test.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dfac5bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from data.data_creator import data_create_SNLI\n",
    "\n",
    "if SHOULD_USE_SEED:\n",
    "    np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "77e0510a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e290f6b",
   "metadata": {},
   "source": [
    "## Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3919a68e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data, labels = data_create_SNLI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4337536",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Splitting the dataset by using the original split\n",
    "\n",
    "x_train, x_val, x_test = data[TRAIN_FILE], data[VAL_FILE], data[TEST_FILE]\n",
    "y_train, y_val, y_test = labels[TRAIN_FILE], labels[VAL_FILE], labels[TEST_FILE]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1c07f7",
   "metadata": {},
   "source": [
    "## Black-box model - Multinomial Naive Bayes classifier\n",
    "\n",
    "The black box model that was used in the original article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c84857d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "data_multinomial_nb = dict()\n",
    "\n",
    "for key in data:\n",
    "    data_multinomial_nb[key] = \\\n",
    "        [first + ', ' + second for first, second in zip(data[key]['premise'], data[key]['hypothesis'])]\n",
    "    \n",
    "x_train, x_val, x_test = data_multinomial_nb[TRAIN_FILE], data_multinomial_nb[VAL_FILE], data_multinomial_nb[TEST_FILE]\n",
    "\n",
    "vect_text = TfidfVectorizer(use_idf = False)\n",
    "x_vec_train = vect_text.fit_transform(x_train)\n",
    "\n",
    "clf = MultinomialNB().fit(x_vec_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb8a419c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf.predict(vect_text.transform(x_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52677dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy 0.5363\n"
     ]
    }
   ],
   "source": [
    "print('Val accuracy', metrics.accuracy_score(y_val, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bb75a5",
   "metadata": {},
   "source": [
    "## Black-box model - Neural Network\n",
    "\n",
    "The black box model above doesn't give a good result. Therefore, we use another black box model presented in the following article:\n",
    "https://nlp.stanford.edu/pubs/snli_paper.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dba8c0a",
   "metadata": {},
   "source": [
    "### Data preperation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d13b45fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://jamesmccaffrey.wordpress.com/2021/01/04/creating-a-custom-torchtext-dataset-from-a-text-file/\n",
    "\n",
    "from torchtext.legacy.data import Field\n",
    "import torchtext as tt\n",
    "\n",
    "TEXT = tt.legacy.data.Field(sequential=True,\n",
    "  init_token='(bos)',  # start of sequence\n",
    "  eos_token='(eos)',   # replace parens with less, greater\n",
    "  lower=True,\n",
    "  tokenize=tt.data.utils.get_tokenizer(\"basic_english\"),)\n",
    "LABEL = tt.legacy.data.Field(sequential=False,\n",
    "  use_vocab=True,\n",
    "  unk_token=None,\n",
    "  is_target=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e220f953",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "(train_obj, valid_obj, test_obj) = tt.legacy.data.TabularDataset.splits(\n",
    "  path=\".//data/SNLI/\",\n",
    "  train=TRAIN_FILE,\n",
    "  validation=VAL_FILE,\n",
    "  test=TEST_FILE,\n",
    "  skip_header = True,\n",
    "  format='csv',\n",
    "  filter_pred = lambda x: x.gold_label != '-',\n",
    "    #gold_label\tsentence1_binary_parse\tsentence2_binary_parse\tsentence1_parse\tsentence2_parse\tsentence1\tsentence2\tcaptionID\tpairID\tlabel1\tlabel2\tlabel3\tlabel4\tlabel5\n",
    "  fields=[('gold_label', LABEL), ('sentence1_binary_parse', None), ('sentence2_binary_parse', None), \n",
    "          ('sentence1_parse', None), ('sentence1_parse', None), ('sentence1', TEXT), ('sentence2', TEXT), ('captionID', None), \n",
    "         ('pairID', None), ('label1', None), ('label2', None), ('label3', None), ('label4', None), ('label5', None)], \n",
    "    csv_reader_params = {'delimiter' : '\t'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0e10115c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_obj.sentence1, min_freq=1, vectors='glove.6B.300d')\n",
    "TEXT.build_vocab(train_obj.sentence2, min_freq=1, vectors='glove.6B.300d')\n",
    "LABEL.build_vocab(train_obj.gold_label)\n",
    "pretrained_embeddings = TEXT.vocab.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "77f3fb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, val_iter, test_iter = tt.legacy.data.BucketIterator.splits(\n",
    "   (train_obj, valid_obj, test_obj), \n",
    "    batch_size=NN_BATCH_SIZE, \n",
    "    sort_key = lambda x: len(x.sentence1),\n",
    "    sort_within_batch = False,\n",
    "    repeat=False, \n",
    "    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e660ce15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Should be in separat file!\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "class NeuralNetModel (nn.Module):\n",
    "    def __init__(self, hidden_size, embeddings, embeddings_size, num_layers, text, tag, num_classes = 3, verbose = True):\n",
    "        super().__init__()\n",
    "        self.text = text\n",
    "        self.tag = tag\n",
    "        self.N = len(tag.vocab.itos)   # tag vocab size\n",
    "        self.V = len(text.vocab.itos)  # text vocab size\n",
    "        self.verbose = verbose\n",
    "\n",
    "        pad_id = self.text.vocab.stoi[self.tag.pad_token]\n",
    "        self.loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.premise_layer = nn.Sequential(\n",
    "            nn.Embedding.from_pretrained(embeddings = embeddings, freeze = True),\n",
    "            nn.LSTM(input_size = embeddings_size, hidden_size = hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        )\n",
    "        self.hypothesis_layer = nn.Sequential(\n",
    "            nn.Embedding.from_pretrained(embeddings = embeddings, freeze = True),\n",
    "            nn.LSTM(input_size = embeddings_size, hidden_size = hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        )\n",
    "        self.model_layers = nn.Sequential(\n",
    "            nn.Linear(hidden_size*2, 200),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(200, 200),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(200, 200),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(200, num_classes)\n",
    "        )\n",
    "        self.log_soft_max = nn.LogSoftmax(dim = -1)\n",
    "\n",
    "    def init_parameters(self, init_low=-0.5, init_high=0.5):\n",
    "        \"\"\"Initialize parameters. We usually use larger initial values for smaller models.\n",
    "        See http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf for a more\n",
    "        in-depth discussion.\n",
    "        \"\"\"\n",
    "        for p in self.parameters():\n",
    "            p.data.uniform_(init_low, init_high)\n",
    "        \n",
    "    def forward(self, premise_batch, hypothesis_batch):\n",
    "        \"\"\"Performs forward computation over a whole text_batch, returns logits.\n",
    "\n",
    "        Arguments: \n",
    "          text_batch: a tensor of size max_length x batch_size\n",
    "          hidden_0: a tensor of size 1 x batch_size x hidden_size\n",
    "        Returns:\n",
    "          logits: a tensor of size max_length x batch_size x N. It provides a logit for each tag of each word of each sentence\n",
    "          in the batch.\n",
    "        \"\"\"\n",
    "        premise_batch = premise_batch.T\n",
    "        hypothesis_batch = hypothesis_batch.T\n",
    "        _, (premise_layer_output, _) = self.premise_layer(premise_batch)\n",
    "        _, (hypothesis_layer_output, _) = self.hypothesis_layer(hypothesis_batch)\n",
    "        premise_layer_output = premise_layer_output[-1, :, :].squeeze(0)\n",
    "        hypothesis_layer_output = hypothesis_layer_output[-1, :, :].squeeze(0)\n",
    "        #print(premise_layer_output.shape)\n",
    "        concat_output = torch.cat((premise_layer_output, hypothesis_layer_output), dim = 1)\n",
    "        #print(concat_output.shape)\n",
    "        res = self.model_layers(concat_output)\n",
    "        logits = self.model_layers(concat_output)\n",
    "        return logits\n",
    "\n",
    "    '''\n",
    "       Computes the loss for a batch by comparing logits of a batch returned by forward to ground_truth, \n",
    "       which stores the true tag ids for the batch. Thus logits is a tensor of size max_length x batch_size x N, \n",
    "       and ground_truth is a tensor of size max_length x batch_size. Note that the criterion functions in torch expect \n",
    "       outputs of a certain shape, so you might need to perform some shape conversions.\n",
    "\n",
    "       You might find nn.CrossEntropyLoss from the last project segment useful. \n",
    "       Note that if you use nn.CrossEntropyLoss then you should not use a softmax layer at the end since that's \n",
    "       already absorbed into the loss function. Alternatively, you can use nn.LogSoftmax as the final sublayer \n",
    "       in the forward pass, but then you need to use nn.NLLLoss, which does not contain its own softmax.\n",
    "       We recommend the former, since working in log space is usually more numerically stable. \n",
    "       For reshaping tensors, check out the torch.Tensor.view method.\n",
    "    '''\n",
    "    def compute_loss(self, logits, ground_truth):\n",
    "        return self.loss_function(logits, ground_truth.view(-1))\n",
    "\n",
    "    '''\n",
    "      Trains the model on training data generated by the iterator train_iter and validation data val_iter.\n",
    "      The epochs and learning_rate variables are the number of epochs (number of times to run through the training data) \n",
    "      to run for and the learning rate for the optimizer, respectively. You can use the validation data to determine \n",
    "      which model was the best one as the epochs go by. Notice that our code below assumes that during training \n",
    "      the best model is stored so that rnn_tagger.load_state_dict(rnn_tagger.best_model) restores the \n",
    "      parameters of the best model.\n",
    "    '''\n",
    "    \n",
    "    def train_all(self, train_iter, val_iter, epochs=10, learning_rate=0.001):\n",
    "        # Switch the module to training mode\n",
    "        self.train()\n",
    "        # Use Adam to optimize the parameters\n",
    "        optim = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        best_validation_accuracy = -float('inf')\n",
    "        best_model = None\n",
    "        # Run the optimization for multiple epochs\n",
    "        for epoch in range(epochs): \n",
    "            total = 0\n",
    "            running_loss = 0.0\n",
    "            for batch in tqdm(train_iter, leave=self.verbose):\n",
    "                # Zero the parameter gradients\n",
    "                self.zero_grad()\n",
    "\n",
    "                # Input and target\n",
    "                premises = batch.sentence1\n",
    "                hypothesis = batch.sentence2 \n",
    "                tags = batch.gold_label\n",
    "                # Run forward pass and compute loss along the way.\n",
    "                \n",
    "                logits = self.forward(premises, hypothesis)\n",
    "                #print(logits.shape)\n",
    "                loss = self.compute_loss(logits, tags)\n",
    "\n",
    "#                 # Perform backpropagation\n",
    "#                 print(logits)\n",
    "#                 print(tags)\n",
    "#                 print(\"#############################################################\")\n",
    "                (loss/premises.size(1)).backward()\n",
    "                #print(loss)\n",
    "\n",
    "                # Update parameters\n",
    "                optim.step()\n",
    "\n",
    "                # Training stats\n",
    "                total += 1\n",
    "                running_loss += loss.item()\n",
    "                \n",
    "            # Evaluate and track improvements on the validation dataset\n",
    "            validation_accuracy = self.evaluate(val_iter)\n",
    "            if validation_accuracy > best_validation_accuracy:\n",
    "                best_validation_accuracy = validation_accuracy\n",
    "                self.best_model = copy.deepcopy(self.state_dict())\n",
    "            epoch_loss = running_loss / total\n",
    "            if (self.verbose):\n",
    "                print (f'Epoch: {epoch} Loss: {epoch_loss:.4f} '\n",
    "                      f'Validation accuracy: {validation_accuracy:.4f}')\n",
    "\n",
    "            \n",
    "    def predict(self, text_batch):\n",
    "        \"\"\"Returns the most likely sequence of tags for a sequence of words in `text_batch`.\n",
    "\n",
    "        Arguments: \n",
    "          text_batch: a tensor containing word ids of size (seq_len, batch_size) \n",
    "        Returns:\n",
    "          tag_batch: a tensor containing tag ids of size (seq_len, batch_size)\n",
    "        \"\"\"\n",
    "        logits = self.forward(text_batch.sentence1, text_batch.sentence2)\n",
    "        #print(logits)\n",
    "        tag_batch = torch.argmax(logits, axis = -1)\n",
    "        return tag_batch\n",
    "\n",
    "\n",
    "    def evaluate(self, iterator):\n",
    "        \"\"\"Returns the model's performance on a given dataset `iterator`.\n",
    "\n",
    "        Arguments: \n",
    "          iterator\n",
    "        Returns:\n",
    "          overall accuracy\n",
    "        \"\"\"\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        pad_id = self.text.vocab.stoi[self.tag.pad_token]\n",
    "        for batch in tqdm(iterator, leave = self.verbose):\n",
    "            premises = batch.sentence1\n",
    "            hypothesis = batch.sentence2 \n",
    "            tags = batch.gold_label\n",
    "            # Run forward pass and compute loss along the way.\n",
    "                \n",
    "            #logits = self.forward(premises, hypothesis)\n",
    "            #tags = batch.tag\n",
    "            tags_pred = self.predict(batch)\n",
    "            mask = tags.ne(pad_id)\n",
    "            cor = (tags == tags_pred)[mask]\n",
    "            correct += cor.float().sum().item()\n",
    "            total += mask.float().sum().item()\n",
    "        return correct/total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901aeb0d",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1cf13bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = NeuralNetModel(100, pretrained_embeddings, 300, 10, TEXT, LABEL).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f9b6c2bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['entailment', 'contradiction', 'neutral']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABEL.vocab.itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4aad2f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 537/537 [01:11<00:00,  7.53it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 29.78it/s]\n",
      "  0%|          | 0/537 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 1.0988 Validation accuracy: 0.5033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 537/537 [01:13<00:00,  7.35it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 29.34it/s]\n",
      "  0%|          | 0/537 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 1.0987 Validation accuracy: 0.5033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 537/537 [01:11<00:00,  7.46it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 29.54it/s]\n",
      "  0%|          | 0/537 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Loss: 1.0987 Validation accuracy: 0.5033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 537/537 [01:11<00:00,  7.48it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 29.58it/s]\n",
      "  0%|          | 0/537 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Loss: 1.0987 Validation accuracy: 0.4967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 537/537 [01:17<00:00,  6.91it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 29.59it/s]\n",
      "  0%|          | 0/537 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Loss: 1.0987 Validation accuracy: 0.4967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 537/537 [01:12<00:00,  7.44it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 29.62it/s]\n",
      "  0%|          | 0/537 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 Loss: 1.0986 Validation accuracy: 0.4967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 537/537 [01:13<00:00,  7.35it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 26.91it/s]\n",
      "  0%|          | 0/537 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 Loss: 1.0986 Validation accuracy: 0.4967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 175/537 [00:24<00:50,  7.17it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-b16f4b7e8a8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-72-0d3a260285c5>\u001b[0m in \u001b[0;36mtrain_all\u001b[0;34m(self, train_iter, val_iter, epochs, learning_rate)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;31m#                 print(tags)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;31m#                 print(\"#############################################################\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mpremises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m                 \u001b[0;31m#print(loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "clf.train_all(train_iter, val_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb1c585",
   "metadata": {},
   "source": [
    "## Instance to explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "075c0997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tensor_to_text (t , FIELD):\n",
    "    if (t.dim() == 0):\n",
    "        return FIELD.vocab.itos[t.item()]\n",
    "    return ' '.join([FIELD.vocab.itos[i] for i in t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "217c2ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "premise to explain:  (bos) two women are standing in a room where others are sitting down at tables and desks . (eos) <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "hypothesis to explain:  (bos) tall humans standing . (eos) <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "tensor([[-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755],\n",
      "        [-1.1230, -1.0980, -1.0755]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Predicted class:  neutral\n",
      "True class:  neutral\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_iter))\n",
    "print('premise to explain: ',convert_tensor_to_text(batch.sentence1[:, 0], TEXT))\n",
    "print('hypothesis to explain: ',convert_tensor_to_text(batch.sentence2[:, 0], TEXT))\n",
    "print('Predicted class: ', convert_tensor_to_text(clf.predict(batch)[0], LABEL))\n",
    "print('True class: ', convert_tensor_to_text(batch.gold_label[0], LABEL))\n",
    "# x_explain = x_test[1]#\"the movie's thesis -- elegant technology for the masses -- is surprisingly refreshing .\"\n",
    "# for batch in train_iter:\n",
    "#     print('x to explain: ',batch)\n",
    "#     print('Predicted class: ', clf.predict(batch))\n",
    "#     print('True class: ', y_test[1])\n",
    "#     print('Predict probablilities: ', clf.predict_proba(vect_text.transform([x_explain]))[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c214e6c8",
   "metadata": {},
   "source": [
    "## Building MeLime model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be40a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from torch.utils.data import DataLoader\n",
    "def tokenizer(x):\n",
    "    return x.split()\n",
    "dl_train = [tokenizer(x) for x in x_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7647c69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_models.word2vec_gen import Word2VecGen, Word2VecEncoder\n",
    "#The radius is <radius> most similar words\n",
    "generator = Word2VecGen(encoder = Word2VecEncoder(dl_train), corpus = x_train, radius = RADIUS, tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3c8433",
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpretable_local_models.statistics_model import StatisticsLocalModel\n",
    "tokenized_x_explain = x_explain.split()\n",
    "y_p_explain = max(clf.predict_proba(vect_text.transform([x_explain]))[0])\n",
    "explainer_model = StatisticsLocalModel(y_p_explain, len(tokenized_x_explain), tokenizer)\n",
    "print(tokenized_x_explain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a880e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MeLime.model import MeLimeModel\n",
    "\n",
    "def transform_func(x):\n",
    "    return vect_text.transform([x])\n",
    "\n",
    "model = MeLimeModel(black_box_model = clf,gen_model =generator, batch_size = BATCH_SIZE, epsilon_c = EPSILON, \n",
    "                    sigma = SIGMA, explainer_model = explainer_model, transform_func = transform_func, \n",
    "                    max_iters = MAX_ITERS, tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b988a94",
   "metadata": {},
   "source": [
    "## Explaining the instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c56f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "res, sentences_with_probs = model.forward(x_explain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ba51b8",
   "metadata": {},
   "source": [
    "## Plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c15fce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ax = StatisticsLocalModel.plot_explaination(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7616e90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "StatisticsLocalModel.plot_sentence_heatmap(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698add19",
   "metadata": {},
   "source": [
    "## Plotting most favorable and contrary samples phrases:\n",
    "\n",
    "Favorable sentence - a generated sentence using Word2VecGen that improves the model's confidence in its \n",
    "prediction on the original sentence.\n",
    "\n",
    "Contrary samples - a generated sentence using Word2VecGen that decrease the model's confidence in its prediction on the original sentence and <b>might even change its prediction on the generated sentence</b>.\n",
    "\n",
    "### Most contrary samples phrases:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9567648b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(sentences_with_probs, key = lambda x: x[1])[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f946d2",
   "metadata": {},
   "source": [
    "### Most favorable samples phrases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd709af",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(sentences_with_probs, key = lambda x: x[1], reverse = True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f9eb53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
