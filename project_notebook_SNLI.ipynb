{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67e6ba05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters:\n",
    "\n",
    "SHOULD_USE_SEED = True\n",
    "RADIUS = 10000\n",
    "BATCH_SIZE = 700\n",
    "EPSILON = 0.1\n",
    "SIGMA = 0.0001\n",
    "MAX_ITERS = 100\n",
    "\n",
    "NN_BATCH_SIZE = 64\n",
    "\n",
    "TRAIN_FILE = \"snli_1.0_train.txt\"\n",
    "VAL_FILE = \"snli_1.0_dev.txt\"\n",
    "TEST_FILE = \"snli_1.0_test.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfac5bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from data.data_creator import data_create_SNLI\n",
    "\n",
    "if SHOULD_USE_SEED:\n",
    "    np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77e0510a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e290f6b",
   "metadata": {},
   "source": [
    "## Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3919a68e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data, labels = data_create_SNLI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4337536",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Splitting the dataset by using the original split\n",
    "\n",
    "x_train, x_val, x_test = data[TRAIN_FILE], data[VAL_FILE], data[TEST_FILE]\n",
    "y_train, y_val, y_test = labels[TRAIN_FILE], labels[VAL_FILE], labels[TEST_FILE]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1c07f7",
   "metadata": {},
   "source": [
    "## Black-box model - Multinomial Naive Bayes classifier\n",
    "\n",
    "The black box model that was used in the original article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c84857d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "data_multinomial_nb = dict()\n",
    "\n",
    "for key in data:\n",
    "    data_multinomial_nb[key] = \\\n",
    "        [first + ', ' + second for first, second in zip(data[key]['premise'], data[key]['hypothesis'])]\n",
    "    \n",
    "x_train, x_val, x_test = data_multinomial_nb[TRAIN_FILE], data_multinomial_nb[VAL_FILE], data_multinomial_nb[TEST_FILE]\n",
    "\n",
    "vect_text = TfidfVectorizer(use_idf = False)\n",
    "x_vec_train = vect_text.fit_transform(x_train)\n",
    "\n",
    "clf = MultinomialNB().fit(x_vec_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb8a419c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf.predict(vect_text.transform(x_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52677dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy 0.5363\n"
     ]
    }
   ],
   "source": [
    "print('Val accuracy', metrics.accuracy_score(y_val, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bb75a5",
   "metadata": {},
   "source": [
    "## Black-box model - Neural Network\n",
    "\n",
    "The black box model above doesn't give a good result. Therefore, we use another black box model presented in the following article:\n",
    "https://nlp.stanford.edu/pubs/snli_paper.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dba8c0a",
   "metadata": {},
   "source": [
    "### Data preperation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d13b45fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://jamesmccaffrey.wordpress.com/2021/01/04/creating-a-custom-torchtext-dataset-from-a-text-file/\n",
    "\n",
    "from torchtext.legacy.data import Field\n",
    "import torchtext as tt\n",
    "\n",
    "TEXT = tt.legacy.data.Field(sequential=True,\n",
    "  init_token='(bos)',  # start of sequence\n",
    "  eos_token='(eos)',   # replace parens with less, greater\n",
    "  lower=True,\n",
    "  tokenize=tt.data.utils.get_tokenizer(\"basic_english\"),)\n",
    "LABEL = tt.legacy.data.Field(sequential=False,\n",
    "  use_vocab=True,\n",
    "  unk_token=None,\n",
    "  is_target=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e220f953",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "(train_obj, valid_obj, test_obj) = tt.legacy.data.TabularDataset.splits(\n",
    "  path=\".//data/SNLI/\",\n",
    "  train=TRAIN_FILE,\n",
    "  validation=VAL_FILE,\n",
    "  test=TEST_FILE,\n",
    "  skip_header = True,\n",
    "  format='csv',\n",
    "  filter_pred = lambda x: x.gold_label != '-',\n",
    "    #gold_label\tsentence1_binary_parse\tsentence2_binary_parse\tsentence1_parse\tsentence2_parse\tsentence1\tsentence2\tcaptionID\tpairID\tlabel1\tlabel2\tlabel3\tlabel4\tlabel5\n",
    "  fields=[('gold_label', LABEL), ('sentence1_binary_parse', None), ('sentence2_binary_parse', None), \n",
    "          ('sentence1_parse', None), ('sentence1_parse', None), ('sentence1', TEXT), ('sentence2', TEXT), ('captionID', None), \n",
    "         ('pairID', None), ('label1', None), ('label2', None), ('label3', None), ('label4', None), ('label5', None)], \n",
    "    csv_reader_params = {'delimiter' : '\t'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e10115c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_obj.sentence1, min_freq=1, vectors='glove.6B.300d')\n",
    "TEXT.build_vocab(train_obj.sentence2, min_freq=1, vectors='glove.6B.300d')\n",
    "LABEL.build_vocab(train_obj.gold_label)\n",
    "pretrained_embeddings = TEXT.vocab.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77f3fb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, val_iter, test_iter = tt.legacy.data.BucketIterator.splits(\n",
    "   (train_obj, valid_obj, test_obj), \n",
    "    batch_size=NN_BATCH_SIZE, \n",
    "    sort_key = lambda x: len(x.sentence1),\n",
    "    sort_within_batch = False,\n",
    "    repeat=False, \n",
    "    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e660ce15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Should be in separat file!\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "class NeuralNetModel (nn.Module):\n",
    "    def __init__(self, hidden_size, embeddings, embeddings_size, num_layers, text, tag, num_classes = 3, verbose = True):\n",
    "        super().__init__()\n",
    "        self.text = text\n",
    "        self.tag = tag\n",
    "        self.N = len(tag.vocab.itos)   # tag vocab size\n",
    "        self.V = len(text.vocab.itos)  # text vocab size\n",
    "        self.verbose = verbose\n",
    "\n",
    "        pad_id = self.text.vocab.stoi[self.tag.pad_token]\n",
    "        self.loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.premise_layer = nn.Sequential(\n",
    "            nn.Embedding.from_pretrained(embeddings = embeddings, freeze = True),\n",
    "            nn.LSTM(input_size = embeddings_size, hidden_size = hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        )\n",
    "        self.hypothesis_layer = nn.Sequential(\n",
    "            nn.Embedding.from_pretrained(embeddings = embeddings, freeze = True),\n",
    "            nn.LSTM(input_size = embeddings_size, hidden_size = hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        )\n",
    "        self.model_layers = nn.Sequential(\n",
    "            nn.Linear(hidden_size*2, 200),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(200, 200),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(200, 200),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(200, num_classes),\n",
    "            nn.Softmax(dim = -1)\n",
    "        )\n",
    "        self.log_soft_max = nn.LogSoftmax(dim = -1)\n",
    "\n",
    "#     def init_parameters(self, init_low=-0.5, init_high=0.5):\n",
    "#         \"\"\"Initialize parameters. We usually use larger initial values for smaller models.\n",
    "#         See http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf for a more\n",
    "#         in-depth discussion.\n",
    "#         \"\"\"\n",
    "#         for p in self.parameters():\n",
    "#             p.data.uniform_(init_low, init_high)\n",
    "        \n",
    "    def forward(self, premise_batch, hypothesis_batch):\n",
    "        \"\"\"Performs forward computation over a whole text_batch, returns logits.\n",
    "\n",
    "        Arguments: \n",
    "          text_batch: a tensor of size max_length x batch_size\n",
    "          hidden_0: a tensor of size 1 x batch_size x hidden_size\n",
    "        Returns:\n",
    "          logits: a tensor of size max_length x batch_size x N. It provides a logit for each tag of each word of each sentence\n",
    "          in the batch.\n",
    "        \"\"\"\n",
    "        premise_batch = premise_batch.T\n",
    "        hypothesis_batch = hypothesis_batch.T\n",
    "        _, (premise_layer_output, _) = self.premise_layer(premise_batch)\n",
    "        _, (hypothesis_layer_output, _) = self.hypothesis_layer(hypothesis_batch)\n",
    "        premise_layer_output = premise_layer_output[-1, :, :].squeeze(0)\n",
    "        hypothesis_layer_output = hypothesis_layer_output[-1, :, :].squeeze(0)\n",
    "        if (len(premise_layer_output.shape) == 1):\n",
    "            premise_layer_output = premise_layer_output.unsqueeze(0)\n",
    "        if (len(hypothesis_layer_output.shape) == 1):\n",
    "            hypothesis_layer_output = hypothesis_layer_output.unsqueeze(0)\n",
    "        #print(premise_layer_output.shape)\n",
    "        concat_output = torch.cat((premise_layer_output, hypothesis_layer_output), dim = 1)\n",
    "        #print(concat_output.shape)\n",
    "        res = self.model_layers(concat_output)\n",
    "        logits = self.model_layers(concat_output)\n",
    "        #print(logits)\n",
    "        return logits\n",
    "\n",
    "    '''\n",
    "       Computes the loss for a batch by comparing logits of a batch returned by forward to ground_truth, \n",
    "       which stores the true tag ids for the batch. Thus logits is a tensor of size max_length x batch_size x N, \n",
    "       and ground_truth is a tensor of size max_length x batch_size. Note that the criterion functions in torch expect \n",
    "       outputs of a certain shape, so you might need to perform some shape conversions.\n",
    "\n",
    "       You might find nn.CrossEntropyLoss from the last project segment useful. \n",
    "       Note that if you use nn.CrossEntropyLoss then you should not use a softmax layer at the end since that's \n",
    "       already absorbed into the loss function. Alternatively, you can use nn.LogSoftmax as the final sublayer \n",
    "       in the forward pass, but then you need to use nn.NLLLoss, which does not contain its own softmax.\n",
    "       We recommend the former, since working in log space is usually more numerically stable. \n",
    "       For reshaping tensors, check out the torch.Tensor.view method.\n",
    "    '''\n",
    "    def compute_loss(self, logits, ground_truth):\n",
    "        return self.loss_function(logits, ground_truth.view(-1))\n",
    "\n",
    "    '''\n",
    "      Trains the model on training data generated by the iterator train_iter and validation data val_iter.\n",
    "      The epochs and learning_rate variables are the number of epochs (number of times to run through the training data) \n",
    "      to run for and the learning rate for the optimizer, respectively. You can use the validation data to determine \n",
    "      which model was the best one as the epochs go by. Notice that our code below assumes that during training \n",
    "      the best model is stored so that rnn_tagger.load_state_dict(rnn_tagger.best_model) restores the \n",
    "      parameters of the best model.\n",
    "    '''\n",
    "    \n",
    "    def train_all(self, train_iter, val_iter, epochs=100, learning_rate=0.001):\n",
    "        # Switch the module to training mode\n",
    "        self.train()\n",
    "        # Use Adam to optimize the parameters\n",
    "        optim = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        best_validation_accuracy = -float('inf')\n",
    "        best_model = None\n",
    "        # Run the optimization for multiple epochs\n",
    "        for epoch in range(epochs): \n",
    "            total = 0\n",
    "            running_loss = 0.0\n",
    "            for batch in tqdm(train_iter, leave=self.verbose):\n",
    "                # Zero the parameter gradients\n",
    "                self.zero_grad()\n",
    "\n",
    "                # Input and target\n",
    "                premises = batch.sentence1\n",
    "                hypothesis = batch.sentence2 \n",
    "                tags = batch.gold_label\n",
    "                # Run forward pass and compute loss along the way.\n",
    "                \n",
    "                logits = self.forward(premises, hypothesis)\n",
    "                #print(logits.shape)\n",
    "                loss = self.compute_loss(logits, tags)\n",
    "\n",
    "#                 # Perform backpropagation\n",
    "#                 print(logits)\n",
    "#                 print(tags)\n",
    "#                 print(\"#############################################################\")\n",
    "                (loss/premises.size(1)).backward()\n",
    "                #print(loss)\n",
    "\n",
    "                # Update parameters\n",
    "                optim.step()\n",
    "\n",
    "                # Training stats\n",
    "                total += 1\n",
    "                running_loss += loss.item()\n",
    "                \n",
    "            # Evaluate and track improvements on the validation dataset\n",
    "            validation_accuracy = self.evaluate(val_iter)\n",
    "            if validation_accuracy > best_validation_accuracy:\n",
    "                best_validation_accuracy = validation_accuracy\n",
    "                self.best_model = copy.deepcopy(self.state_dict())\n",
    "            epoch_loss = running_loss / total\n",
    "            if (self.verbose):\n",
    "                print (f'Epoch: {epoch} Loss: {epoch_loss:.4f} '\n",
    "                      f'Validation accuracy: {validation_accuracy:.4f}')\n",
    "\n",
    "            \n",
    "    def predict(self, text_batch):\n",
    "        \"\"\"Returns the most likely sequence of tags for a sequence of words in `text_batch`.\n",
    "\n",
    "        Arguments: \n",
    "          text_batch: a tensor containing word ids of size (seq_len, batch_size) \n",
    "        Returns:\n",
    "          tag_batch: a tensor containing tag ids of size (seq_len, batch_size)\n",
    "        \"\"\"\n",
    "        logits = self.forward(text_batch.sentence1, text_batch.sentence2)\n",
    "        #print(logits)\n",
    "        tag_batch = torch.argmax(logits, axis = -1)\n",
    "        return tag_batch\n",
    "    \n",
    "    def predict_proba(self, text_batch):\n",
    "        \"\"\"Returns the most likely sequence of tags for a sequence of words in `text_batch`.\n",
    "\n",
    "        Arguments: \n",
    "          text_batch: a tensor containing word ids of size (seq_len, batch_size) \n",
    "        Returns:\n",
    "          tag_batch: a tensor containing tag ids of size (seq_len, batch_size)\n",
    "        \"\"\"\n",
    "        if (text_batch is list):\n",
    "            print(\":)\")\n",
    "        logits = self.forward(text_batch.sentence1, text_batch.sentence2).detach().cpu().numpy()\n",
    "        return logits\n",
    "\n",
    "\n",
    "    def evaluate(self, iterator):\n",
    "        \"\"\"Returns the model's performance on a given dataset `iterator`.\n",
    "\n",
    "        Arguments: \n",
    "          iterator\n",
    "        Returns:\n",
    "          overall accuracy\n",
    "        \"\"\"\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        pad_id = self.text.vocab.stoi[self.tag.pad_token]\n",
    "        for batch in tqdm(iterator, leave = self.verbose):\n",
    "            premises = batch.sentence1\n",
    "            hypothesis = batch.sentence2 \n",
    "            tags = batch.gold_label\n",
    "            # Run forward pass and compute loss along the way.\n",
    "                \n",
    "            #logits = self.forward(premises, hypothesis)\n",
    "            #tags = batch.tag\n",
    "            tags_pred = self.predict(batch)\n",
    "            mask = tags.ne(pad_id)\n",
    "            cor = (tags == tags_pred)[mask]\n",
    "            correct += cor.float().sum().item()\n",
    "            total += mask.float().sum().item()\n",
    "        return correct/total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901aeb0d",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cf13bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = NeuralNetModel(100, pretrained_embeddings, 300, 1, TEXT, LABEL).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9b6c2bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['entailment', 'contradiction', 'neutral']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABEL.vocab.itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aad2f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8584/8584 [01:22<00:00, 104.50it/s]\n",
      "100%|██████████| 154/154 [00:00<00:00, 245.99it/s]\n",
      "  0%|          | 0/8584 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 0.9416 Validation accuracy: 0.6361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8584/8584 [01:20<00:00, 106.54it/s]\n",
      "100%|██████████| 154/154 [00:00<00:00, 285.98it/s]\n",
      "  0%|          | 0/8584 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 0.8945 Validation accuracy: 0.5865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8584/8584 [01:18<00:00, 109.48it/s]\n",
      "100%|██████████| 154/154 [00:00<00:00, 289.52it/s]\n",
      "  0%|          | 0/8584 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Loss: 0.8810 Validation accuracy: 0.6355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8584/8584 [01:16<00:00, 111.59it/s]\n",
      "100%|██████████| 154/154 [00:00<00:00, 289.78it/s]\n",
      "  0%|          | 0/8584 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Loss: 0.8713 Validation accuracy: 0.6538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 1974/8584 [00:17<00:53, 122.76it/s]"
     ]
    }
   ],
   "source": [
    "clf.train_all(train_iter, val_iter, epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb1c585",
   "metadata": {},
   "source": [
    "## Instance to explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075c0997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tensor_to_text (t , FIELD):\n",
    "    if (t.dim() == 0):\n",
    "        return FIELD.vocab.itos[t.item()]\n",
    "    return ' '.join([FIELD.vocab.itos[i] for i in t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217c2ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_iter))\n",
    "print('premise to explain: ',convert_tensor_to_text(batch.sentence1[:, 0], TEXT))\n",
    "print('hypothesis to explain: ',convert_tensor_to_text(batch.sentence2[:, 0], TEXT))\n",
    "print('Predicted class: ', convert_tensor_to_text(clf.predict(batch)[0], LABEL))\n",
    "print('True class: ', convert_tensor_to_text(batch.gold_label[0], LABEL))\n",
    "# x_explain = x_test[1]#\"the movie's thesis -- elegant technology for the masses -- is surprisingly refreshing .\"\n",
    "# for batch in train_iter:\n",
    "#     print('x to explain: ',batch)\n",
    "#     print('Predicted class: ', clf.predict(batch))\n",
    "#     print('True class: ', y_test[1])\n",
    "#     print('Predict probablilities: ', clf.predict_proba(vect_text.transform([x_explain]))[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c214e6c8",
   "metadata": {},
   "source": [
    "## Building MeLime model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be40a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from torch.utils.data import DataLoader\n",
    "def tokenizer(x):\n",
    "#     if '*' in x:\n",
    "#         x = x.split('*')\n",
    "#         return x[0].split() + x[1].split()\n",
    "    return x.split()\n",
    "dl_train = [tokenizer(x) for x in x_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7647c69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_models.word2vec_gen import Word2VecGen, Word2VecEncoder\n",
    "#The radius is <radius> most similar words\n",
    "generator = Word2VecGen(encoder = Word2VecEncoder(dl_train), corpus = x_train, radius = RADIUS, tokenizer = tokenizer,\n",
    "                       tokens_not_to_sample = ['*', '.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3c8433",
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpretable_local_models.statistics_model import StatisticsLocalModel\n",
    "y_p_explain = max(clf.predict_proba(batch)[0]).item()\n",
    "print('Probability for the predicted label: ', y_p_explain)\n",
    "print('The tokenized hypothesis:')\n",
    "\n",
    "tokenized_x_explain_premise = tokenizer(convert_tensor_to_text(batch.sentence1[:, 0], TEXT))\n",
    "tokenized_x_explain_hypothesis = tokenizer(convert_tensor_to_text(batch.sentence2[:, 0], TEXT))\n",
    "fliter_out_tokens = [TEXT.pad_token, TEXT.init_token, TEXT.eos_token]\n",
    "tokenized_x_explain_hypothesis = list(filter(lambda x: x not in fliter_out_tokens, tokenized_x_explain_hypothesis))\n",
    "tokenized_x_explain_premise = list(filter(lambda x: x not in fliter_out_tokens, tokenized_x_explain_premise))\n",
    "print(tokenized_x_explain_hypothesis)\n",
    "x_explain_hypothesis = ' '.join(tokenized_x_explain_hypothesis)\n",
    "x_explain_premise = ' '.join(tokenized_x_explain_premise)\n",
    "\n",
    "x_explain = x_explain_premise + \" * \" + x_explain_hypothesis\n",
    "print(x_explain)\n",
    "explainer_model = StatisticsLocalModel(y_p_explain, len(tokenizer(x_explain)), tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a880e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MeLime.model import MeLimeModel\n",
    "from torch import tensor\n",
    "\n",
    "class Instance:\n",
    "    \n",
    "    def __init__(self, sentence1, sentence2):\n",
    "        self.sentence1 = sentence1\n",
    "        self.sentence2 = sentence2\n",
    "\n",
    "premise_len = batch.sentence1.shape[0]\n",
    "hypothesis_len = batch.sentence2.shape[0]\n",
    "\n",
    "def create_tensor_from_sentence(sentence, length):\n",
    "    pad_id = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "    init_id = TEXT.vocab.stoi[TEXT.init_token]\n",
    "    eos_id = TEXT.vocab.stoi[TEXT.eos_token]\n",
    "    tensor = torch.ones((2,), dtype=torch.int64)\n",
    "    t = tensor.new_full(size = (length, 1), fill_value  = pad_id, device = device)\n",
    "    t[0, 0] = init_id\n",
    "    tokens_idx = torch.LongTensor([TEXT.vocab.stoi[token] for token in tokenizer(sentence)])\n",
    "    t[1 : len(tokens_idx) + 1, 0] = tokens_idx\n",
    "    t[len(tokens_idx) + 1, 0] = eos_id\n",
    "    return t\n",
    "\n",
    "def transform_func(x):\n",
    "    splitted_x = x.split('*')\n",
    "    premise = splitted_x[0]\n",
    "    hypothesis = splitted_x[1]\n",
    "    t_premise = create_tensor_from_sentence(premise, premise_len)\n",
    "    t_hypothesis = create_tensor_from_sentence(hypothesis, hypothesis_len)\n",
    "#     print(t_premise)\n",
    "#     print(t_hypothesis)\n",
    "#     print(premise)\n",
    "#     print(hypothesis)\n",
    "    return Instance(t_premise, t_hypothesis)\n",
    "    #return vect_text.transform([x])\n",
    "\n",
    "model = MeLimeModel(black_box_model = clf,gen_model =generator, batch_size = BATCH_SIZE, epsilon_c = EPSILON, \n",
    "                    sigma = SIGMA, explainer_model = explainer_model, transform_func = transform_func, \n",
    "                    max_iters = MAX_ITERS, tokenizer = tokenizer)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b988a94",
   "metadata": {},
   "source": [
    "## Explaining the instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c56f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "res, sentences_with_probs = model.forward(x_explain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ba51b8",
   "metadata": {},
   "source": [
    "## Plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c15fce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ax = StatisticsLocalModel.plot_explaination(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7616e90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "premise_res = []\n",
    "hypothesis_res = []\n",
    "did_finish_premise = False\n",
    "for word, stat in res:\n",
    "    if word == '*':\n",
    "        did_finish_premise = True\n",
    "        continue\n",
    "    if did_finish_premise:\n",
    "        hypothesis_res.append((word, stat))\n",
    "        continue\n",
    "    premise_res.append((word, stat))\n",
    "print(\"Premise plot:\")\n",
    "StatisticsLocalModel.plot_sentence_heatmap(premise_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8001d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hypothesis plot:\")\n",
    "StatisticsLocalModel.plot_sentence_heatmap(hypothesis_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698add19",
   "metadata": {},
   "source": [
    "## Plotting most favorable and contrary samples phrases:\n",
    "\n",
    "Favorable sentence - a generated sentence using Word2VecGen that improves the model's confidence in its \n",
    "prediction on the original sentence.\n",
    "\n",
    "Contrary samples - a generated sentence using Word2VecGen that decrease the model's confidence in its prediction on the original sentence and <b>might even change its prediction on the generated sentence</b>.\n",
    "\n",
    "### Most contrary samples phrases:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9567648b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(sentences_with_probs, key = lambda x: x[1])[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f946d2",
   "metadata": {},
   "source": [
    "### Most favorable samples phrases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd709af",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(sentences_with_probs, key = lambda x: x[1], reverse = True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f9eb53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
