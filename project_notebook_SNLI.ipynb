{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67e6ba05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters:\n",
    "\n",
    "SHOULD_USE_SEED = True\n",
    "RADIUS = 5000\n",
    "BATCH_SIZE = 700\n",
    "EPSILON = 0.1\n",
    "SIGMA = 0.001\n",
    "MAX_ITERS = 100\n",
    "\n",
    "NN_BATCH_SIZE = 128\n",
    "\n",
    "TRAIN_FILE = \"esnli_train.csv\"\n",
    "VAL_FILE = \"esnli_dev.csv\"\n",
    "TEST_FILE = \"esnli_test.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfac5bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from data.data_creator import data_create_SNLI\n",
    "\n",
    "if SHOULD_USE_SEED:\n",
    "    np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77e0510a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e290f6b",
   "metadata": {},
   "source": [
    "## Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3919a68e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data, labels = data_create_SNLI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4337536",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Splitting the dataset by using the original split\n",
    "\n",
    "x_train, x_val, x_test = data[TRAIN_FILE], data[VAL_FILE], data[TEST_FILE]\n",
    "y_train, y_val, y_test = labels[TRAIN_FILE], labels[VAL_FILE], labels[TEST_FILE]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1c07f7",
   "metadata": {},
   "source": [
    "## Black-box model - Multinomial Naive Bayes classifier\n",
    "\n",
    "The black box model that was used in the original article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c84857d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "data_multinomial_nb = dict()\n",
    "\n",
    "for key in data:\n",
    "    data_multinomial_nb[key] = \\\n",
    "        [first + '~ ' + second for first, second in zip(data[key]['premise'], data[key]['hypothesis'])]\n",
    "    \n",
    "x_train, x_val, x_test = data_multinomial_nb[TRAIN_FILE], data_multinomial_nb[VAL_FILE], data_multinomial_nb[TEST_FILE]\n",
    "\n",
    "vect_text = TfidfVectorizer(use_idf = False)\n",
    "x_vec_train = vect_text.fit_transform(x_train)\n",
    "\n",
    "clf = MultinomialNB().fit(x_vec_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb8a419c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf.predict(vect_text.transform(x_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52677dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy 0.5420646210119894\n"
     ]
    }
   ],
   "source": [
    "print('Val accuracy', metrics.accuracy_score(y_val, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bb75a5",
   "metadata": {},
   "source": [
    "## Black-box model - Neural Network\n",
    "\n",
    "The black box model above doesn't give a good result. Therefore, we use another black box model presented in the following article:\n",
    "https://nlp.stanford.edu/pubs/snli_paper.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dba8c0a",
   "metadata": {},
   "source": [
    "### Data preperation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d13b45fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://jamesmccaffrey.wordpress.com/2021/01/04/creating-a-custom-torchtext-dataset-from-a-text-file/\n",
    "\n",
    "from torchtext.legacy.data import Field\n",
    "import torchtext as tt\n",
    "\n",
    "TEXT = tt.legacy.data.Field(sequential=True,\n",
    "  init_token='(bos)',  # start of sequence\n",
    "  eos_token='(eos)',   # replace parens with less, greater\n",
    "  lower=True,\n",
    "  tokenize=tt.data.utils.get_tokenizer(\"basic_english\"),)\n",
    "LABEL = tt.legacy.data.Field(sequential=False,\n",
    "  use_vocab=True,\n",
    "  unk_token=None,\n",
    "  is_target=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e220f953",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-7193ed845629>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m (train_obj, valid_obj, test_obj) = tt.legacy.data.TabularDataset.splits(\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\".//data/eSNLI/\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTRAIN_FILE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torchtext/legacy/data/dataset.py\u001b[0m in \u001b[0;36msplits\u001b[0;34m(cls, path, root, train, validation, test, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         train_data = None if train is None else cls(\n\u001b[0m\u001b[1;32m     78\u001b[0m             os.path.join(path, train), **kwargs)\n\u001b[1;32m     79\u001b[0m         val_data = None if validation is None else cls(\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torchtext/legacy/data/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, format, fields, skip_header, csv_reader_params, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0mexamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmake_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torchtext/legacy/data/dataset.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    272\u001b[0m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0mexamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmake_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torchtext/legacy/data/example.py\u001b[0m in \u001b[0;36mfromCSV\u001b[0;34m(cls, data, fields, field_to_index)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfromCSV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield_to_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfield_to_index\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torchtext/legacy/data/example.py\u001b[0m in \u001b[0;36mfromlist\u001b[0;34m(cls, data, fields)\u001b[0m\n\u001b[1;32m     82\u001b[0m                         \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                     \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torchtext/legacy/data/field.py\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_vocab\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_words\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torchtext/legacy/data/pipeline.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \"\"\"\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpipe\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torchtext/legacy/data/pipeline.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \"\"\"\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torchtext/legacy/data/pipeline.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \"\"\"\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "(train_obj, valid_obj, test_obj) = tt.legacy.data.TabularDataset.splits(\n",
    "  path=\".//data/eSNLI/\",\n",
    "  train=TRAIN_FILE,\n",
    "  validation=VAL_FILE,\n",
    "  test=TEST_FILE,\n",
    "  skip_header = True,\n",
    "  format='csv',\n",
    "  filter_pred = lambda x: x.gold_label != '-',\n",
    "    #gold_label\tsentence1_binary_parse\tsentence2_binary_parse\tsentence1_parse\tsentence2_parse\tsentence1\tsentence2\tcaptionID\tpairID\tlabel1\tlabel2\tlabel3\tlabel4\tlabel5\n",
    "  fields=[('pairID', None), ('gold_label', LABEL), ('sentence1', TEXT), ('sentence2', TEXT)])#, ('sentence1_binary_parse', None), ('sentence2_binary_parse', None), \n",
    "          #('sentence1_parse', None), ('sentence1_parse', None), ('captionID', None), \n",
    "         #('pairID', None), ('label1', None), ('label2', None), ('label3', None), ('label4', None), ('label5', None)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e10115c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_obj.sentence1, min_freq=1, vectors='glove.6B.300d')\n",
    "TEXT.build_vocab(train_obj.sentence2, min_freq=1, vectors='glove.6B.300d')\n",
    "LABEL.build_vocab(train_obj.gold_label)\n",
    "pretrained_embeddings = TEXT.vocab.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f3fb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, val_iter, test_iter = tt.legacy.data.BucketIterator.splits(\n",
    "   (train_obj, valid_obj, test_obj), \n",
    "    batch_size=NN_BATCH_SIZE, \n",
    "    sort_key = lambda x: len(x.sentence1),\n",
    "    sort_within_batch = False,\n",
    "    repeat=False, \n",
    "    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e660ce15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #TODO: Should be in separat file!\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from tqdm import tqdm\n",
    "# import copy\n",
    "\n",
    "# class NeuralNetModel (nn.Module):\n",
    "#     def __init__(self, hidden_size, embeddings, embeddings_size, num_layers, text, tag, num_classes = 3, verbose = True):\n",
    "#         super().__init__()\n",
    "#         self.text = text\n",
    "#         self.tag = tag\n",
    "#         self.N = len(tag.vocab.itos)   # tag vocab size\n",
    "#         self.V = len(text.vocab.itos)  # text vocab size\n",
    "#         self.verbose = verbose\n",
    "\n",
    "#         pad_id = self.text.vocab.stoi[self.tag.pad_token]\n",
    "#         self.loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "#         self.premise_layer = nn.Sequential(\n",
    "#             nn.Embedding.from_pretrained(embeddings = embeddings, freeze = True),\n",
    "#             nn.LSTM(input_size = embeddings_size, hidden_size = hidden_size, num_layers=num_layers, batch_first=True)\n",
    "#         )\n",
    "#         self.hypothesis_layer = nn.Sequential(\n",
    "#             nn.Embedding.from_pretrained(embeddings = embeddings, freeze = True),\n",
    "#             nn.LSTM(input_size = embeddings_size, hidden_size = hidden_size, num_layers=num_layers, batch_first=True)\n",
    "#         )\n",
    "#         self.model_layers = nn.Sequential(\n",
    "#             nn.Linear(hidden_size*2, 200),\n",
    "#             nn.Tanh(),\n",
    "#             nn.Linear(200, 200),\n",
    "#             nn.Tanh(),\n",
    "#             nn.Linear(200, 200),\n",
    "#             nn.Tanh(),\n",
    "#             nn.Linear(200, num_classes),\n",
    "#             nn.Softmax(dim = -1)\n",
    "#         )\n",
    "#         self.log_soft_max = nn.LogSoftmax(dim = -1)\n",
    "\n",
    "# #     def init_parameters(self, init_low=-0.5, init_high=0.5):\n",
    "# #         \"\"\"Initialize parameters. We usually use larger initial values for smaller models.\n",
    "# #         See http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf for a more\n",
    "# #         in-depth discussion.\n",
    "# #         \"\"\"\n",
    "# #         for p in self.parameters():\n",
    "# #             p.data.uniform_(init_low, init_high)\n",
    "        \n",
    "#     def forward(self, premise_batch, hypothesis_batch):\n",
    "#         \"\"\"Performs forward computation over a whole text_batch, returns logits.\n",
    "\n",
    "#         Arguments: \n",
    "#           text_batch: a tensor of size max_length x batch_size\n",
    "#           hidden_0: a tensor of size 1 x batch_size x hidden_size\n",
    "#         Returns:\n",
    "#           logits: a tensor of size max_length x batch_size x N. It provides a logit for each tag of each word of each sentence\n",
    "#           in the batch.\n",
    "#         \"\"\"\n",
    "#         premise_batch = premise_batch.T\n",
    "#         hypothesis_batch = hypothesis_batch.T\n",
    "#         _, (premise_layer_output, _) = self.premise_layer(premise_batch)\n",
    "#         _, (hypothesis_layer_output, _) = self.hypothesis_layer(hypothesis_batch)\n",
    "#         premise_layer_output = premise_layer_output[-1, :, :].squeeze(0)\n",
    "#         hypothesis_layer_output = hypothesis_layer_output[-1, :, :].squeeze(0)\n",
    "#         if (len(premise_layer_output.shape) == 1):\n",
    "#             premise_layer_output = premise_layer_output.unsqueeze(0)\n",
    "#         if (len(hypothesis_layer_output.shape) == 1):\n",
    "#             hypothesis_layer_output = hypothesis_layer_output.unsqueeze(0)\n",
    "#         #print(premise_layer_output.shape)\n",
    "#         concat_output = torch.cat((premise_layer_output, hypothesis_layer_output), dim = 1)\n",
    "#         #print(concat_output.shape)\n",
    "#         res = self.model_layers(concat_output)\n",
    "#         logits = self.model_layers(concat_output)\n",
    "#         #print(logits)\n",
    "#         return logits\n",
    "\n",
    "#     '''\n",
    "#        Computes the loss for a batch by comparing logits of a batch returned by forward to ground_truth, \n",
    "#        which stores the true tag ids for the batch. Thus logits is a tensor of size max_length x batch_size x N, \n",
    "#        and ground_truth is a tensor of size max_length x batch_size. Note that the criterion functions in torch expect \n",
    "#        outputs of a certain shape, so you might need to perform some shape conversions.\n",
    "\n",
    "#        You might find nn.CrossEntropyLoss from the last project segment useful. \n",
    "#        Note that if you use nn.CrossEntropyLoss then you should not use a softmax layer at the end since that's \n",
    "#        already absorbed into the loss function. Alternatively, you can use nn.LogSoftmax as the final sublayer \n",
    "#        in the forward pass, but then you need to use nn.NLLLoss, which does not contain its own softmax.\n",
    "#        We recommend the former, since working in log space is usually more numerically stable. \n",
    "#        For reshaping tensors, check out the torch.Tensor.view method.\n",
    "#     '''\n",
    "#     def compute_loss(self, logits, ground_truth):\n",
    "#         return self.loss_function(logits, ground_truth.view(-1))\n",
    "\n",
    "#     '''\n",
    "#       Trains the model on training data generated by the iterator train_iter and validation data val_iter.\n",
    "#       The epochs and learning_rate variables are the number of epochs (number of times to run through the training data) \n",
    "#       to run for and the learning rate for the optimizer, respectively. You can use the validation data to determine \n",
    "#       which model was the best one as the epochs go by. Notice that our code below assumes that during training \n",
    "#       the best model is stored so that rnn_tagger.load_state_dict(rnn_tagger.best_model) restores the \n",
    "#       parameters of the best model.\n",
    "#     '''\n",
    "    \n",
    "#     def train_all(self, train_iter, val_iter, epochs=100, learning_rate=0.001):\n",
    "#         # Switch the module to training mode\n",
    "#         self.train()\n",
    "#         # Use Adam to optimize the parameters\n",
    "#         optim = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "#         best_validation_accuracy = -float('inf')\n",
    "#         best_model = None\n",
    "#         # Run the optimization for multiple epochs\n",
    "#         for epoch in range(epochs): \n",
    "#             total = 0\n",
    "#             running_loss = 0.0\n",
    "#             for batch in tqdm(train_iter, leave=self.verbose):\n",
    "#                 # Zero the parameter gradients\n",
    "#                 self.zero_grad()\n",
    "\n",
    "#                 # Input and target\n",
    "#                 premises = batch.sentence1\n",
    "#                 hypothesis = batch.sentence2 \n",
    "#                 tags = batch.gold_label\n",
    "#                 # Run forward pass and compute loss along the way.\n",
    "                \n",
    "#                 logits = self.forward(premises, hypothesis)\n",
    "#                 #print(logits.shape)\n",
    "#                 loss = self.compute_loss(logits, tags)\n",
    "\n",
    "# #                 # Perform backpropagation\n",
    "# #                 print(logits)\n",
    "# #                 print(tags)\n",
    "# #                 print(\"#############################################################\")\n",
    "#                 (loss/premises.size(1)).backward()\n",
    "#                 #print(loss)\n",
    "\n",
    "#                 # Update parameters\n",
    "#                 optim.step()\n",
    "\n",
    "#                 # Training stats\n",
    "#                 total += 1\n",
    "#                 running_loss += loss.item()\n",
    "                \n",
    "#             # Evaluate and track improvements on the validation dataset\n",
    "#             validation_accuracy = self.evaluate(val_iter)\n",
    "#             if validation_accuracy > best_validation_accuracy:\n",
    "#                 best_validation_accuracy = validation_accuracy\n",
    "#                 self.best_model = copy.deepcopy(self.state_dict())\n",
    "#             epoch_loss = running_loss / total\n",
    "#             if (self.verbose):\n",
    "#                 print (f'Epoch: {epoch} Loss: {epoch_loss:.4f} '\n",
    "#                       f'Validation accuracy: {validation_accuracy:.4f}')\n",
    "\n",
    "            \n",
    "#     def predict(self, text_batch):\n",
    "#         \"\"\"Returns the most likely sequence of tags for a sequence of words in `text_batch`.\n",
    "\n",
    "#         Arguments: \n",
    "#           text_batch: a tensor containing word ids of size (seq_len, batch_size) \n",
    "#         Returns:\n",
    "#           tag_batch: a tensor containing tag ids of size (seq_len, batch_size)\n",
    "#         \"\"\"\n",
    "#         logits = self.forward(text_batch.sentence1, text_batch.sentence2)\n",
    "#         #print(logits)\n",
    "#         tag_batch = torch.argmax(logits, axis = -1)\n",
    "#         return tag_batch\n",
    "    \n",
    "#     def predict_proba(self, text_batch):\n",
    "#         \"\"\"Returns the most likely sequence of tags for a sequence of words in `text_batch`.\n",
    "\n",
    "#         Arguments: \n",
    "#           text_batch: a tensor containing word ids of size (seq_len, batch_size) \n",
    "#         Returns:\n",
    "#           tag_batch: a tensor containing tag ids of size (seq_len, batch_size)\n",
    "#         \"\"\"\n",
    "#         if (text_batch is list):\n",
    "#             print(\":)\")\n",
    "#         logits = self.forward(text_batch.sentence1, text_batch.sentence2).detach().cpu().numpy()\n",
    "#         return logits\n",
    "\n",
    "\n",
    "#     def evaluate(self, iterator):\n",
    "#         \"\"\"Returns the model's performance on a given dataset `iterator`.\n",
    "\n",
    "#         Arguments: \n",
    "#           iterator\n",
    "#         Returns:\n",
    "#           overall accuracy\n",
    "#         \"\"\"\n",
    "#         correct = 0\n",
    "#         total = 0\n",
    "#         pad_id = self.text.vocab.stoi[self.tag.pad_token]\n",
    "#         for batch in tqdm(iterator, leave = self.verbose):\n",
    "#             premises = batch.sentence1\n",
    "#             hypothesis = batch.sentence2 \n",
    "#             tags = batch.gold_label\n",
    "#             # Run forward pass and compute loss along the way.\n",
    "                \n",
    "#             #logits = self.forward(premises, hypothesis)\n",
    "#             #tags = batch.tag\n",
    "#             tags_pred = self.predict(batch)\n",
    "#             mask = tags.ne(pad_id)\n",
    "#             cor = (tags == tags_pred)[mask]\n",
    "#             correct += cor.float().sum().item()\n",
    "#             total += mask.float().sum().item()\n",
    "#         return correct/total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901aeb0d",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf13bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from black_box_models.neural_network import NeuralNetModel\n",
    "\n",
    "clf = NeuralNetModel(100, pretrained_embeddings, 300, 1, TEXT, LABEL).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b6c2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL.vocab.itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aad2f5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf.train_all(train_iter, val_iter, epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb1c585",
   "metadata": {},
   "source": [
    "## Instance to explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075c0997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tensor_to_text (t , FIELD):\n",
    "    if (t.dim() == 0):\n",
    "        return FIELD.vocab.itos[t.item()]\n",
    "    return ' '.join([FIELD.vocab.itos[i] for i in t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec4f2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_iter))\n",
    "\n",
    "def create_tensor_from_sentence(sentence):\n",
    "    length = len(tokenizer(sentence)) + 2\n",
    "    pad_id = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "    init_id = TEXT.vocab.stoi[TEXT.init_token]\n",
    "    eos_id = TEXT.vocab.stoi[TEXT.eos_token]\n",
    "    tensor = torch.ones((2,), dtype=torch.int64)\n",
    "    t = tensor.new_full(size = (length, 1), fill_value  = pad_id, device = device)\n",
    "    t[0, 0] = init_id\n",
    "    tokens_idx = torch.LongTensor([TEXT.vocab.stoi[token] for token in tokenizer(sentence)])\n",
    "    t[1 : len(tokens_idx) + 1, 0] = tokens_idx\n",
    "    t[len(tokens_idx) + 1, 0] = eos_id\n",
    "    return t\n",
    "\n",
    "class Instance:\n",
    "    \n",
    "    def __init__(self, sentence1, sentence2):\n",
    "        self.sentence1 = sentence1\n",
    "        self.sentence2 = sentence2\n",
    "\n",
    "def transform_func(x):\n",
    "    splitted_x = x.split('*')\n",
    "    premise = splitted_x[0]\n",
    "    hypothesis = splitted_x[1]\n",
    "    t_premise = create_tensor_from_sentence(premise)\n",
    "    t_hypothesis = create_tensor_from_sentence(hypothesis)\n",
    "#     print(t_premise)\n",
    "#     print(t_hypothesis)\n",
    "#     print(premise)\n",
    "#     print(hypothesis)\n",
    "    return Instance(t_premise, t_hypothesis)\n",
    "    #return vect_text.transform([x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217c2ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = next(iter(train_iter))\n",
    "# print('premise to explain: ',convert_tensor_to_text(batch.sentence1[:, 0], TEXT))\n",
    "# print('hypothesis to explain: ',convert_tensor_to_text(batch.sentence2[:, 0], TEXT))\n",
    "# print('Predicted class: ', convert_tensor_to_text(clf.predict(batch)[0], LABEL))\n",
    "# print('True class: ', convert_tensor_to_text(batch.gold_label[0], LABEL))\n",
    "def tokenizer(x):\n",
    "#     if '*' in x:\n",
    "#         x = x.split('*')\n",
    "#         return x[0].split() + x[1].split()\n",
    "    return x.split()\n",
    "\n",
    "\n",
    "for idx in range(len(x_test)):\n",
    "    x_explain = x_test[idx]#\"the movie's thesis -- elegant technology for the masses -- is surprisingly refreshing .\"\n",
    "    premise = x_test[idx].split('~')[0]\n",
    "    hypothesis = x_test[idx].split('~')[1]\n",
    "#     \n",
    "    x_explain = premise + \" * \" + hypothesis \n",
    "    if(convert_tensor_to_text(clf.predict(transform_func(x_explain))[0], LABEL) == y_test[idx] and y_test[idx] != 'neutral'):\n",
    "        print('premise to explain: ', premise)\n",
    "        print('hypothesis to explain: ',hypothesis)\n",
    "        print('Predicted class: ', convert_tensor_to_text(clf.predict(transform_func(x_explain))[0], LABEL))\n",
    "        print('True class: ', y_test[idx])\n",
    "        break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c214e6c8",
   "metadata": {},
   "source": [
    "## Building MeLime model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be40a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dl_train = [tokenizer(x) for x in x_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7647c69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_models.word2vec_gen import Word2VecGen, Word2VecEncoder\n",
    "#The radius is <radius> most similar words\n",
    "encoder = Word2VecEncoder(dl_train)\n",
    "generator = Word2VecGen(encoder = encoder, corpus = x_train, radius = RADIUS, tokenizer = tokenizer,\n",
    "                       tokens_not_to_sample = ['*', '.', 'a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3c8433",
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpretable_local_models.statistics_model_nli import StatisticsLocalModelNLI\n",
    "y_p_explain = max(clf.predict_proba(transform_func(x_explain))[0]).item()\n",
    "print('Probability for the predicted label: ', y_p_explain)\n",
    "# print('The tokenized hypothesis:')\n",
    "\n",
    "# tokenized_x_explain_premise = tokenizer(convert_tensor_to_text(batch.sentence1[:, 0], TEXT))\n",
    "# tokenized_x_explain_hypothesis = tokenizer(convert_tensor_to_text(batch.sentence2[:, 0], TEXT))\n",
    "# fliter_out_tokens = [TEXT.pad_token, TEXT.init_token, TEXT.eos_token]\n",
    "# tokenized_x_explain_hypothesis = list(filter(lambda x: x not in fliter_out_tokens, tokenized_x_explain_hypothesis))\n",
    "# tokenized_x_explain_premise = list(filter(lambda x: x not in fliter_out_tokens, tokenized_x_explain_premise))\n",
    "# print(tokenized_x_explain_hypothesis)\n",
    "# x_explain_hypothesis = ' '.join(tokenized_x_explain_hypothesis)\n",
    "# x_explain_premise = ' '.join(tokenized_x_explain_premise)\n",
    "\n",
    "# x_explain = x_explain_premise + \" * \" + x_explain_hypothesis\n",
    "# print(x_explain)\n",
    "print(tokenizer(x_explain))\n",
    "print(tokenizer(premise))\n",
    "explainer_model = StatisticsLocalModelNLI(y_p_explain, len(tokenizer(x_explain)), tokenizer, \n",
    "                                       len(tokenizer(premise)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a880e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MeLime.model import MeLimeModel\n",
    "from torch import tensor\n",
    "\n",
    "\n",
    "model = MeLimeModel(black_box_model = clf,gen_model =generator, batch_size = BATCH_SIZE, epsilon_c = EPSILON, \n",
    "                    sigma = SIGMA, explainer_model = explainer_model, transform_func = transform_func, \n",
    "                    max_iters = MAX_ITERS, tokenizer = tokenizer)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b988a94",
   "metadata": {},
   "source": [
    "## Explaining the instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c56f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "res, sentences_with_probs = model.forward(x_explain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ba51b8",
   "metadata": {},
   "source": [
    "## Plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c15fce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ax = StatisticsLocalModelNLI.plot_explaination(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7616e90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "premise_res = []\n",
    "hypothesis_res = []\n",
    "did_finish_premise = False\n",
    "for word, stat in res:\n",
    "    if word == '*':\n",
    "        did_finish_premise = True\n",
    "        continue\n",
    "    if did_finish_premise:\n",
    "        hypothesis_res.append((word, stat))\n",
    "        continue\n",
    "    premise_res.append((word, stat))\n",
    "print(\"Premise plot:\")\n",
    "StatisticsLocalModelNLI.plot_sentence_heatmap(premise_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21005ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hypothesis plot:\")\n",
    "StatisticsLocalModelNLI.plot_sentence_heatmap(hypothesis_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698add19",
   "metadata": {},
   "source": [
    "## Plotting most favorable and contrary samples phrases:\n",
    "\n",
    "Favorable sentence - a generated sentence using Word2VecGen that improves the model's confidence in its \n",
    "prediction on the original sentence.\n",
    "\n",
    "Contrary samples - a generated sentence using Word2VecGen that decrease the model's confidence in its prediction on the original sentence and <b>might even change its prediction on the generated sentence</b>.\n",
    "\n",
    "### Most contrary samples phrases:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9567648b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(sentences_with_probs, key = lambda x: x[1])[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f946d2",
   "metadata": {},
   "source": [
    "### Most favorable samples phrases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd709af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted(sentences_with_probs, key = lambda x: x[1], reverse = True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f9eb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MeLime.measures import calc_f1_esnli\n",
    "\n",
    "F1_score = calc_f1_esnli(convert_tensor_to_text, clf, transform_func, LABEL, y_p_explain, tokenizer, encoder, \n",
    "                                          x_train, RADIUS, BATCH_SIZE, EPSILON, SIGMA, MAX_ITERS, None)\n",
    "print(\"F1 score: \", F1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df04e19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from os.path import isfile, join\n",
    "\n",
    "# # Getting gold explanations:\n",
    "# df = pd.read_csv(join(\"data/eSNLI\", 'esnli_test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848bbd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_sentence_explanation(df, num):\n",
    "#     x_explain = []\n",
    "#     for i in range(len(df['Sentence' + str(num) + '_Highlighted_1'])):\n",
    "#         res = None\n",
    "#         for t in ['1', '2', '3']:\n",
    "#             x = df['Sentence' + str(num) + '_Highlighted_'+t][i]\n",
    "#             if x == '{}':\n",
    "#                 res = set()\n",
    "#             else:\n",
    "#                 if res == None:\n",
    "#                     res = set(map(int, x.split(',')))\n",
    "#                     continue\n",
    "#                 res = res.intersection(set(map(int, x.split(','))))\n",
    "#         x_explain.append(res)\n",
    "#     return x_explain\n",
    "\n",
    "# premise_explanations = get_sentence_explanation(df, 1)\n",
    "# hypothesis_explanations = get_sentence_explanation(df, 2)\n",
    "# threshold = 0.3\n",
    "# num_samples = 0\n",
    "# tp = 0\n",
    "# fp = 0\n",
    "# fn = 0\n",
    "\n",
    "# for i, (premise_explanation, hypothesis_explanation) in enumerate(zip(premise_explanations, hypothesis_explanations)):\n",
    "#     premise = df['Sentence1'][i]\n",
    "#     hypothesis = df['Sentence2'][i]\n",
    "#     x_explain = premise + ' * ' + hypothesis\n",
    "#     label = df['gold_label'][i]\n",
    "#     if(convert_tensor_to_text(clf.predict(transform_func(x_explain))[0], LABEL) == label):\n",
    "# #         print(\"Computing\")\n",
    "# #         print(x_explain)\n",
    "#         id_seperator = len(tokenizer(premise))\n",
    "#         #print(id_seperator)\n",
    "#         explainer_model = StatisticsLocalModelNLI(y_p_explain, len(tokenizer(x_explain)), tokenizer, id_seperator)\n",
    "#         generator = Word2VecGen(encoder = encoder, corpus = x_train, radius = RADIUS, tokenizer = tokenizer,\n",
    "#                        tokens_not_to_sample = ['*', '.', 'a'])\n",
    "#         model = MeLimeModel(black_box_model = clf,gen_model =generator, batch_size = BATCH_SIZE, epsilon_c = EPSILON, \n",
    "#                     sigma = SIGMA, explainer_model = explainer_model, transform_func = transform_func, \n",
    "#                     max_iters = MAX_ITERS, tokenizer = tokenizer)\n",
    "#         res, sentences_with_probs = model.forward(x_explain, False)\n",
    "#         curr_hypothesis_explanation = set()\n",
    "#         curr_premise_explanation = set()\n",
    "#         did_pass_premise = False\n",
    "#         curr_premise_explanation = set([i for i, (word, prob) in enumerate(res) if (prob >= threshold and i < id_seperator)])\n",
    "#         curr_hypothesis_explanation = set([i for i, (word, prob) in enumerate(res) if (prob >= threshold and i > id_seperator)])\n",
    "#         tp += len(curr_premise_explanation.intersection(premise_explanations[i]))\n",
    "#         tp += len(curr_hypothesis_explanation.intersection(hypothesis_explanations[i]))\n",
    "#         fp += len(curr_hypothesis_explanation.difference(hypothesis_explanations[i]))\n",
    "#         fp += len(curr_premise_explanation.difference(premise_explanations[i]))\n",
    "#         fn += len(hypothesis_explanations[i].difference(curr_hypothesis_explanation))\n",
    "#         fn += len(premise_explanations[i].difference(curr_premise_explanation))\n",
    "#         num_samples += 1\n",
    "# F1 = 0 if (tp + 0.5 *(tp + fn)) == 0 else tp / (tp + 0.5 *(tp + fn))\n",
    "# print(F1)\n",
    "#     #res, sentences_with_probs = model.forward(x_explain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adacbd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
