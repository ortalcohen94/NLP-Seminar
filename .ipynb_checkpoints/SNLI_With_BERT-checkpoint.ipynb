{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r68nhCG2P9k2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uiBBZ-KtQW-c",
    "outputId": "aaa804b8-f4a4-4731-83b7-99ed740b3d8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
      "/content/gdrive/My Drive/Colab Notebooks/NLP-Seminar\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "\n",
    "# %cd /content/gdrive/My Drive/Colab Notebooks/NLP-Seminar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "ZvutXYhDQEos"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('esnli_dev.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "cRrWPPLpQPLL"
   },
   "outputs": [],
   "source": [
    "data_sentences = data[['Sentence1', 'Sentence2', 'gold_label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IXKUs124Q87q",
    "outputId": "14d6bd9c-da01-4af1-b31d-66f905e06e4c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:4174: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "labelencoder = LabelEncoder()\n",
    "\n",
    "data_sentences['gold_label_cat'] = labelencoder.fit_transform(data_sentences['gold_label'])\n",
    "data_sentences.drop('gold_label', inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jscsif8JTtgY",
    "outputId": "ae02d1ce-24a7-41c5-84b5-9996362e15b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "fjZhYwNZuSzx"
   },
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "LmJYtGTgCyA3"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "def preprocessing_for_bert(data):\n",
    "\n",
    "    input_ids_1 = []\n",
    "    attention_masks_1 = []\n",
    "\n",
    "    input_ids_2 = []\n",
    "    attention_masks_2 = []\n",
    "\n",
    "    sentence1 = data.Sentence1.values\n",
    "    sentence2 = data.Sentence2.values\n",
    "\n",
    "    for sent in sentence1:\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=text_preprocessing(sent),  \n",
    "            add_special_tokens=True,        \n",
    "            max_length=MAX_LEN,                 \n",
    "            pad_to_max_length=True,         \n",
    "            #return_tensors='pt',           \n",
    "            return_attention_mask=True      \n",
    "            )\n",
    "        \n",
    "        input_ids_1.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks_1.append(encoded_sent.get('attention_mask'))\n",
    "\n",
    "\n",
    "    for sent in sentence2:\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=text_preprocessing(sent),  \n",
    "            add_special_tokens=True,        \n",
    "            max_length=MAX_LEN,                 \n",
    "            pad_to_max_length=True,         \n",
    "            #return_tensors='pt',           \n",
    "            return_attention_mask=True      \n",
    "            )\n",
    "        \n",
    "        input_ids_2.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks_2.append(encoded_sent.get('attention_mask'))        \n",
    "\n",
    "    input_ids_1 = torch.tensor(input_ids_1)\n",
    "    attention_masks_1 = torch.tensor(attention_masks_1)\n",
    "\n",
    "    input_ids_2 = torch.tensor(input_ids_2)\n",
    "    attention_masks_2 = torch.tensor(attention_masks_2)\n",
    "\n",
    "    return input_ids_1, attention_masks_1, input_ids_2, attention_masks_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "036MzCWBvvUR"
   },
   "outputs": [],
   "source": [
    "Sentence1 = data_sentences['Sentence1']\n",
    "Sentence2 = data_sentences['Sentence2']\n",
    "\n",
    "res = pd.concat([Sentence1, Sentence2])\n",
    "res.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "9q9_gJGL0dE9"
   },
   "outputs": [],
   "source": [
    "encoded_sents = [tokenizer.encode(sent, add_special_tokens=True) for sent in res]\n",
    "\n",
    "max_len = max([len(sent) for sent in encoded_sents])\n",
    "# print('Max length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "0g9g_dXj0zBv"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, val = train_test_split(data_sentences, test_size=0.2)\n",
    "\n",
    "X_train, y_train = train[['Sentence1', 'Sentence2']], train['gold_label_cat']\n",
    "X_val, y_val = val[['Sentence1', 'Sentence2']], val['gold_label_cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yrxNjgGl2LS9",
    "outputId": "26410961-bb48-4e34-e7c9-8ab301e7284d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 63\n",
    "\n",
    "train_inputs_1, train_masks_1, train_inputs_2, train_masks_2 = preprocessing_for_bert(X_train)\n",
    "val_inputs_1, val_masks_1, val_inputs_2, val_masks_2 = preprocessing_for_bert(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "v7Ig3ZM1Nn56"
   },
   "outputs": [],
   "source": [
    "combined_train_inputs = torch.cat([train_inputs_1, train_inputs_2], dim = 1)\n",
    "combined_train_masks = torch.cat([train_masks_1, train_masks_2], dim = 1)\n",
    "\n",
    "combined_val_inputs = torch.cat([val_inputs_1, val_inputs_2], dim = 1)\n",
    "combined_val_masks = torch.cat([val_masks_1, val_masks_2], dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "9ekA2QnSKjHj"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Convert other data types to torch.Tensor\n",
    "train_labels = torch.tensor(y_train.values)\n",
    "val_labels = torch.tensor(y_val.values)\n",
    "\n",
    "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(combined_train_inputs, combined_train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set\n",
    "val_data = TensorDataset(combined_val_inputs, combined_val_masks, val_labels)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZwNCXyo2U9Um",
    "outputId": "fa25b115-9b0a-40d5-cfbb-c7cfa81fac7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39 µs, sys: 0 ns, total: 39 µs\n",
      "Wall time: 42.4 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "# Create the BertClassfier class\n",
    "class BertClassifier(nn.Module):\n",
    "    \"\"\"Bert Model for Classification Tasks.\n",
    "    \"\"\"\n",
    "    def __init__(self, freeze_bert=False):\n",
    "        \"\"\"\n",
    "        @param    bert: a BertModel object\n",
    "        @param    classifier: a torch.nn.Module classifier\n",
    "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
    "        \"\"\"\n",
    "        super(BertClassifier, self).__init__()\n",
    "        D_in, H, D_out = 768, 50, 3\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(D_in, H),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(0.5),\n",
    "            nn.Linear(H, D_out)\n",
    "        )\n",
    "\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Feed input to BERT and the classifier to compute logits.\n",
    "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
    "                      max_length)\n",
    "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
    "                      information with shape (batch_size, max_length)\n",
    "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
    "                      num_labels)\n",
    "        \"\"\"\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "        \n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "ct7mREbSdgZU"
   },
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "def initialize_model(epochs=4):\n",
    "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
    "    \"\"\"\n",
    "    bert_classifier = BertClassifier(freeze_bert=False)\n",
    "\n",
    "    bert_classifier.to(device)\n",
    "\n",
    "    optimizer = AdamW(bert_classifier.parameters(),\n",
    "                      lr=5e-5,   \n",
    "                      eps=1e-8   \n",
    "                      )\n",
    "\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0, \n",
    "                                                num_training_steps=total_steps)\n",
    "    return bert_classifier, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "wOny3INSd6IQ"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
    "\n",
    "    print(\"Start training...\\n\")\n",
    "    for epoch_i in range(epochs):\n",
    "\n",
    "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
    "        print(\"-\"*70)\n",
    "\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts +=1\n",
    "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "\n",
    "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        print(\"-\"*70)\n",
    "\n",
    "        if evaluation == True:\n",
    "\n",
    "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
    "\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            \n",
    "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "            print(\"-\"*70)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
    "    on our validation set.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "\n",
    "    for batch in val_dataloader:\n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        val_accuracy.append(accuracy)\n",
    "\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "glOn2U9deIvB",
    "outputId": "88288a0d-d1d2-47b8-e73a-55aef6587d99"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   1.098948   |     -      |     -     |   14.44  \n",
      "   1    |   40    |   1.024430   |     -      |     -     |   14.14  \n",
      "   1    |   60    |   0.862769   |     -      |     -     |   13.85  \n",
      "   1    |   80    |   0.868596   |     -      |     -     |   13.60  \n",
      "   1    |   100   |   0.773322   |     -      |     -     |   13.40  \n",
      "   1    |   120   |   0.754628   |     -      |     -     |   13.42  \n",
      "   1    |   140   |   0.745995   |     -      |     -     |   13.58  \n",
      "   1    |   160   |   0.750236   |     -      |     -     |   13.76  \n",
      "   1    |   180   |   0.697930   |     -      |     -     |   13.81  \n",
      "   1    |   200   |   0.623260   |     -      |     -     |   13.73  \n",
      "   1    |   220   |   0.640764   |     -      |     -     |   13.60  \n",
      "   1    |   240   |   0.627276   |     -      |     -     |   13.55  \n",
      "   1    |   246   |   1.187579   |     -      |     -     |   3.46   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.799949   |  0.585143  |   77.00   |  182.83  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   2    |   20    |   0.560377   |     -      |     -     |   14.28  \n",
      "   2    |   40    |   0.519861   |     -      |     -     |   13.65  \n",
      "   2    |   60    |   0.449581   |     -      |     -     |   13.67  \n",
      "   2    |   80    |   0.501599   |     -      |     -     |   13.64  \n",
      "   2    |   100   |   0.490226   |     -      |     -     |   13.67  \n",
      "   2    |   120   |   0.477949   |     -      |     -     |   13.63  \n",
      "   2    |   140   |   0.483377   |     -      |     -     |   13.64  \n",
      "   2    |   160   |   0.470241   |     -      |     -     |   13.63  \n",
      "   2    |   180   |   0.524005   |     -      |     -     |   13.60  \n",
      "   2    |   200   |   0.396373   |     -      |     -     |   13.62  \n",
      "   2    |   220   |   0.491531   |     -      |     -     |   13.62  \n",
      "   2    |   240   |   0.490512   |     -      |     -     |   13.63  \n",
      "   2    |   246   |   0.414065   |     -      |     -     |   3.48   \n",
      "----------------------------------------------------------------------\n",
      "   2    |    -    |   0.486467   |  0.537487  |   80.41   |  182.33  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   3    |   20    |   0.318850   |     -      |     -     |   14.31  \n",
      "   3    |   40    |   0.275149   |     -      |     -     |   13.62  \n",
      "   3    |   60    |   0.259669   |     -      |     -     |   13.62  \n",
      "   3    |   80    |   0.338888   |     -      |     -     |   13.62  \n",
      "   3    |   100   |   0.351592   |     -      |     -     |   13.60  \n",
      "   3    |   120   |   0.299898   |     -      |     -     |   13.64  \n",
      "   3    |   140   |   0.276820   |     -      |     -     |   13.61  \n",
      "   3    |   160   |   0.316133   |     -      |     -     |   13.59  \n",
      "   3    |   180   |   0.231057   |     -      |     -     |   13.62  \n",
      "   3    |   200   |   0.309068   |     -      |     -     |   13.63  \n",
      "   3    |   220   |   0.298024   |     -      |     -     |   13.63  \n",
      "   3    |   240   |   0.322283   |     -      |     -     |   13.62  \n",
      "   3    |   246   |   0.223441   |     -      |     -     |   3.49   \n",
      "----------------------------------------------------------------------\n",
      "   3    |    -    |   0.298009   |  0.549637  |   81.12   |  182.16  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   4    |   20    |   0.212757   |     -      |     -     |   14.28  \n",
      "   4    |   40    |   0.166769   |     -      |     -     |   13.61  \n",
      "   4    |   60    |   0.161208   |     -      |     -     |   13.59  \n",
      "   4    |   80    |   0.206530   |     -      |     -     |   13.60  \n",
      "   4    |   100   |   0.212861   |     -      |     -     |   13.60  \n",
      "   4    |   120   |   0.240544   |     -      |     -     |   13.62  \n",
      "   4    |   140   |   0.134140   |     -      |     -     |   13.59  \n",
      "   4    |   160   |   0.162967   |     -      |     -     |   13.62  \n",
      "   4    |   180   |   0.182501   |     -      |     -     |   13.61  \n",
      "   4    |   200   |   0.199974   |     -      |     -     |   13.61  \n",
      "   4    |   220   |   0.195310   |     -      |     -     |   13.61  \n",
      "   4    |   240   |   0.183806   |     -      |     -     |   13.61  \n",
      "   4    |   246   |   0.516804   |     -      |     -     |   3.48   \n",
      "----------------------------------------------------------------------\n",
      "   4    |    -    |   0.196360   |  0.654868  |   81.42   |  182.05  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)  \n",
    "bert_classifier, optimizer, scheduler = initialize_model(epochs=4)\n",
    "train(bert_classifier, train_dataloader, val_dataloader, epochs=4, evaluation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "q9iUhR5kgB-Q"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "SNLI With BERT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
